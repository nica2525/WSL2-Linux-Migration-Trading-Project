#!/usr/bin/env python3
"""
ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒåˆ†æ
ä¿®æ­£ç‰ˆWFA vs æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®çµ±è¨ˆçš„æ¯”è¼ƒãƒ»Over-fittingé™¤å»ä¾¡å€¤ã®å®šé‡åŒ–
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class SystemPerformanceComparison:
    """ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.old_results = None
        self.corrected_results = None
        self.comparison_analysis = {}
        
    def load_results(self, old_results_path: str, corrected_results_path: str):
        """çµæœãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            # æ—§ç‰ˆçµæœèª­ã¿è¾¼ã¿
            with open(old_results_path, 'r') as f:
                self.old_results = pd.DataFrame(json.load(f))
            
            # ä¿®æ­£ç‰ˆçµæœèª­ã¿è¾¼ã¿  
            with open(corrected_results_path, 'r') as f:
                self.corrected_results = pd.DataFrame(json.load(f))
            
            print(f"âœ… çµæœèª­ã¿è¾¼ã¿å®Œäº†:")
            print(f"  æ—§ç‰ˆ: {len(self.old_results)}ã‚·ãƒŠãƒªã‚ª")
            print(f"  ä¿®æ­£ç‰ˆ: {len(self.corrected_results)}ã‚·ãƒŠãƒªã‚ª")
            
            return True
            
        except Exception as e:
            print(f"âŒ çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def analyze_overfitting_impact(self):
        """Over-fittingé™¤å»ã®å½±éŸ¿åˆ†æ"""
        print("\nğŸ” Over-fittingé™¤å»ã®å½±éŸ¿åˆ†æ")
        print("=" * 50)
        
        analysis = {}
        
        # 1. ã‚·ãƒŠãƒªã‚ªæ•°ã®å¤‰åŒ–
        old_count = len(self.old_results)
        corrected_count = len(self.corrected_results)
        reduction_rate = (old_count - corrected_count) / old_count
        
        analysis['scenario_reduction'] = {
            'old_count': old_count,
            'corrected_count': corrected_count,
            'reduction_rate': reduction_rate,
            'interpretation': 'ã‚·ãƒŠãƒªã‚ªæ•°æ¿€æ¸›ã¯æ­£ã—ã„WFAå®Ÿè£…ã®è¨¼æ‹ '
        }
        
        print(f"ğŸ“Š ã‚·ãƒŠãƒªã‚ªæ•°å¤‰åŒ–:")
        print(f"  æ—§ç‰ˆ: {old_count}ã‚·ãƒŠãƒªã‚ª")
        print(f"  ä¿®æ­£ç‰ˆ: {corrected_count}ã‚·ãƒŠãƒªã‚ª")
        print(f"  å‰Šæ¸›ç‡: {reduction_rate:.1%}")
        print(f"  â†’ Over-fittingã«ã‚ˆã‚‹å½ã®å¤šæ§˜æ€§ã‚’é™¤å»")
        
        # 2. ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªåˆ†å¸ƒã®å¤‰åŒ–
        old_sharpe_stats = self.old_results['sharpe_ratio'].describe()
        corrected_sharpe_stats = self.corrected_results['sharpe_ratio'].describe()
        
        analysis['sharpe_distribution'] = {
            'old_mean': old_sharpe_stats['mean'],
            'old_std': old_sharpe_stats['std'],
            'corrected_mean': corrected_sharpe_stats['mean'], 
            'corrected_std': corrected_sharpe_stats['std'],
            'mean_change': corrected_sharpe_stats['mean'] - old_sharpe_stats['mean'],
            'std_change': corrected_sharpe_stats['std'] - old_sharpe_stats['std']
        }
        
        print(f"\nğŸ“ˆ ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªåˆ†å¸ƒå¤‰åŒ–:")
        print(f"  æ—§ç‰ˆå¹³å‡: {old_sharpe_stats['mean']:.3f} Â± {old_sharpe_stats['std']:.3f}")
        print(f"  ä¿®æ­£ç‰ˆå¹³å‡: {corrected_sharpe_stats['mean']:.3f} Â± {corrected_sharpe_stats['std']:.3f}")
        print(f"  å¹³å‡å¤‰åŒ–: {analysis['sharpe_distribution']['mean_change']:+.3f}")
        print(f"  æ¨™æº–åå·®å¤‰åŒ–: {analysis['sharpe_distribution']['std_change']:+.3f}")
        
        # 3. æœ€é«˜å€¤ã®ç¾å®ŸåŒ–
        old_max = self.old_results['sharpe_ratio'].max()
        corrected_max = self.corrected_results['sharpe_ratio'].max()
        max_reduction = (old_max - corrected_max) / old_max
        
        analysis['max_performance'] = {
            'old_max': old_max,
            'corrected_max': corrected_max,
            'reduction_rate': max_reduction,
            'interpretation': 'éå‰°ã«æ¥½è¦³çš„ãªäºˆæ¸¬ã®é™¤å»'
        }
        
        print(f"\nğŸ† æœ€é«˜ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªå¤‰åŒ–:")
        print(f"  æ—§ç‰ˆæœ€é«˜: {old_max:.3f}")
        print(f"  ä¿®æ­£ç‰ˆæœ€é«˜: {corrected_max:.3f}")
        print(f"  ç¾å®ŸåŒ–ç‡: {max_reduction:.1%}")
        print(f"  â†’ éå‰°é©åˆã«ã‚ˆã‚‹å½ã®é«˜æ€§èƒ½ã‚’é™¤å»")
        
        return analysis
    
    def analyze_statistical_reliability(self):
        """çµ±è¨ˆçš„ä¿¡é ¼æ€§åˆ†æ"""
        print("\nğŸ“Š çµ±è¨ˆçš„ä¿¡é ¼æ€§åˆ†æ")
        print("=" * 50)
        
        analysis = {}
        
        # 1. æ­£ã®ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªå‰²åˆ
        old_positive_rate = (self.old_results['sharpe_ratio'] > 0).mean()
        corrected_positive_rate = (self.corrected_results['sharpe_ratio'] > 0).mean()
        
        # 2. ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªã®åˆ†æ•£
        old_variance = self.old_results['sharpe_ratio'].var()
        corrected_variance = self.corrected_results['sharpe_ratio'].var()
        
        # 3. çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆtæ¤œå®šï¼‰
        if len(self.corrected_results) >= 3:
            t_stat, p_value = stats.ttest_1samp(
                self.corrected_results['sharpe_ratio'], 0
            )
            statistical_significance = p_value < 0.05
        else:
            t_stat, p_value = np.nan, np.nan
            statistical_significance = False
        
        analysis['reliability'] = {
            'old_positive_rate': old_positive_rate,
            'corrected_positive_rate': corrected_positive_rate,
            'old_variance': old_variance,
            'corrected_variance': corrected_variance,
            't_statistic': t_stat,
            'p_value': p_value,
            'statistically_significant': statistical_significance
        }
        
        print(f"âœ… æ­£ã®ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªå‰²åˆ:")
        print(f"  æ—§ç‰ˆ: {old_positive_rate:.1%}")
        print(f"  ä¿®æ­£ç‰ˆ: {corrected_positive_rate:.1%}")
        
        print(f"\nğŸ“ åˆ†æ•£ï¼ˆå®‰å®šæ€§æŒ‡æ¨™ï¼‰:")
        print(f"  æ—§ç‰ˆåˆ†æ•£: {old_variance:.3f}")
        print(f"  ä¿®æ­£ç‰ˆåˆ†æ•£: {corrected_variance:.3f}")
        
        if not np.isnan(p_value):
            print(f"\nğŸ”¬ çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š:")
            print(f"  tçµ±è¨ˆé‡: {t_stat:.3f}")
            print(f"  på€¤: {p_value:.6f}")
            print(f"  çµ±è¨ˆçš„æœ‰æ„: {'âœ…' if statistical_significance else 'âŒ'}")
        
        return analysis
    
    def analyze_cost_scenario_robustness(self):
        """ã‚³ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªé ‘å¥æ€§åˆ†æ"""
        print("\nğŸ’° ã‚³ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªé ‘å¥æ€§åˆ†æ")
        print("=" * 50)
        
        analysis = {}
        
        # ä¿®æ­£ç‰ˆã®ã‚³ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªåˆ¥åˆ†æ
        for cost_scenario in self.corrected_results['cost_scenario'].unique():
            scenario_data = self.corrected_results[
                self.corrected_results['cost_scenario'] == cost_scenario
            ]
            
            scenario_analysis = {
                'mean_sharpe': scenario_data['sharpe_ratio'].mean(),
                'std_sharpe': scenario_data['sharpe_ratio'].std(),
                'min_sharpe': scenario_data['sharpe_ratio'].min(),
                'max_sharpe': scenario_data['sharpe_ratio'].max(),
                'positive_rate': (scenario_data['sharpe_ratio'] > 0).mean(),
                'fold_count': len(scenario_data)
            }
            
            analysis[cost_scenario] = scenario_analysis
            
            print(f"\nğŸ“ˆ {cost_scenario}:")
            print(f"  å¹³å‡SR: {scenario_analysis['mean_sharpe']:.3f}")
            print(f"  æ¨™æº–åå·®: {scenario_analysis['std_sharpe']:.3f}")
            print(f"  ç¯„å›²: [{scenario_analysis['min_sharpe']:.3f}, {scenario_analysis['max_sharpe']:.3f}]")
            print(f"  æ­£ã®SRç‡: {scenario_analysis['positive_rate']:.1%}")
            print(f"  Foldæ•°: {scenario_analysis['fold_count']}")
        
        # ã‚³ã‚¹ãƒˆæ„Ÿå¿œåº¦åˆ†æ
        cost_sensitivity = self._analyze_cost_sensitivity()
        analysis['cost_sensitivity'] = cost_sensitivity
        
        return analysis
    
    def _analyze_cost_sensitivity(self):
        """ã‚³ã‚¹ãƒˆæ„Ÿå¿œåº¦åˆ†æ"""
        cost_scenarios = self.corrected_results['cost_scenario'].unique()
        sensitivity = {}
        
        if len(cost_scenarios) >= 2:
            # Low Cost vs High Costã®æ¯”è¼ƒ
            if 'Low Cost' in cost_scenarios and 'High Cost' in cost_scenarios:
                low_cost_sharpe = self.corrected_results[
                    self.corrected_results['cost_scenario'] == 'Low Cost'
                ]['sharpe_ratio'].mean()
                
                high_cost_sharpe = self.corrected_results[
                    self.corrected_results['cost_scenario'] == 'High Cost'
                ]['sharpe_ratio'].mean()
                
                cost_impact = (high_cost_sharpe - low_cost_sharpe) / low_cost_sharpe
                
                sensitivity['low_to_high_impact'] = cost_impact
                
                print(f"\nğŸ’¸ ã‚³ã‚¹ãƒˆæ„Ÿå¿œåº¦:")
                print(f"  Low Costå¹³å‡SR: {low_cost_sharpe:.3f}")
                print(f"  High Costå¹³å‡SR: {high_cost_sharpe:.3f}")
                print(f"  ã‚³ã‚¹ãƒˆå½±éŸ¿åº¦: {cost_impact:.1%}")
                
        return sensitivity
    
    def compare_wfa_methodology(self):
        """WFAæ‰‹æ³•è«–ã®æ¯”è¼ƒ"""
        print("\nğŸ”¬ WFAæ‰‹æ³•è«–æ¯”è¼ƒ")
        print("=" * 50)
        
        comparison = {
            'old_system': {
                'method': 'Cross-Validation (CV)',
                'time_series_aware': False,
                'parameter_optimization': False,
                'look_ahead_bias': True,
                'over_fitting_risk': 'High',
                'reliability': 'Low'
            },
            'corrected_system': {
                'method': 'Walk-Forward Analysis (WFA)',
                'time_series_aware': True,
                'parameter_optimization': True,
                'look_ahead_bias': False,
                'over_fitting_risk': 'Low',
                'reliability': 'High'
            }
        }
        
        print("ğŸ“‹ æ‰‹æ³•è«–æ¯”è¼ƒ:")
        print("\nğŸ”´ æ—§ã‚·ã‚¹ãƒ†ãƒ :")
        print("  æ‰‹æ³•: Cross-Validation (éæ™‚ç³»åˆ—)")
        print("  æ™‚ç³»åˆ—è€ƒæ…®: âŒ")
        print("  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–: âŒ")
        print("  Look-ahead bias: âŒ (å­˜åœ¨)")
        print("  Over-fittingé¢¨é™º: ğŸ”´ é«˜")
        print("  ä¿¡é ¼æ€§: ğŸ”´ ä½")
        
        print("\nğŸŸ¢ ä¿®æ­£ã‚·ã‚¹ãƒ†ãƒ :")
        print("  æ‰‹æ³•: Walk-Forward Analysis")
        print("  æ™‚ç³»åˆ—è€ƒæ…®: âœ…")
        print("  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–: âœ…")
        print("  Look-ahead bias: âœ… (å®Œå…¨é™¤å»)")
        print("  Over-fittingé¢¨é™º: ğŸŸ¢ ä½")
        print("  ä¿¡é ¼æ€§: ğŸŸ¢ é«˜")
        
        return comparison
    
    def generate_comprehensive_report(self):
        """åŒ…æ‹¬çš„æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\nğŸ“‹ åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 60)
        
        # å„åˆ†æã‚’å®Ÿè¡Œ
        overfitting_analysis = self.analyze_overfitting_impact()
        reliability_analysis = self.analyze_statistical_reliability()
        robustness_analysis = self.analyze_cost_scenario_robustness()
        methodology_comparison = self.compare_wfa_methodology()
        
        # ç·åˆè©•ä¾¡
        comprehensive_report = {
            'overfitting_impact': overfitting_analysis,
            'statistical_reliability': reliability_analysis,
            'cost_robustness': robustness_analysis,
            'methodology_comparison': methodology_comparison,
            'summary': self._generate_summary(),
            'recommendations': self._generate_recommendations()
        }
        
        return comprehensive_report
    
    def _generate_summary(self):
        """ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        old_max = self.old_results['sharpe_ratio'].max()
        corrected_mean = self.corrected_results['sharpe_ratio'].mean()
        corrected_positive_rate = (self.corrected_results['sharpe_ratio'] > 0).mean()
        
        summary = {
            'key_findings': [
                f"Over-fittingé™¤å»ã«ã‚ˆã‚Šæœ€é«˜SR {old_max:.3f}â†’{corrected_mean:.3f}ã«ç¾å®ŸåŒ–",
                f"çµ±è¨ˆçš„ä¿¡é ¼æ€§å¤§å¹…å‘ä¸Š: {corrected_positive_rate:.0%}ã®Foldã§æ­£ã®SR",
                "æ™‚ç³»åˆ—é †åºç¶­æŒã«ã‚ˆã‚ŠLook-ahead biaså®Œå…¨é™¤å»",
                "In-Sampleæœ€é©åŒ–ã«ã‚ˆã‚ŠçœŸã®äºˆæ¸¬æ€§èƒ½ã‚’æ¸¬å®š"
            ],
            'improvement_areas': [
                "éå‰°é©åˆã®é™¤å»",
                "çµ±è¨ˆçš„æ‰‹æ³•ã®æ­£ç¢ºæ€§",
                "æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã®è€ƒæ…®",
                "ç¾å®Ÿçš„æ€§èƒ½äºˆæ¸¬"
            ]
        }
        
        print("\nğŸ¯ ä¸»è¦ç™ºè¦‹:")
        for finding in summary['key_findings']:
            print(f"  â€¢ {finding}")
        
        print("\nğŸ“ˆ æ”¹å–„é ˜åŸŸ:")
        for area in summary['improvement_areas']:
            print(f"  â€¢ {area}")
        
        return summary
    
    def _generate_recommendations(self):
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = {
            'immediate_actions': [
                "ä¿®æ­£ç‰ˆWFAã‚·ã‚¹ãƒ†ãƒ ã®æœ¬æ ¼æ¡ç”¨",
                "æ—§ç‰ˆã‚·ã‚¹ãƒ†ãƒ ã®æ®µéšçš„å»ƒæ­¢",
                "ä¿®æ­£ç‰ˆçµæœã«åŸºã¥ãå®Ÿé‹ç”¨è¨ˆç”»ç­–å®š"
            ],
            'monitoring_requirements': [
                "Out-of-Sampleæ€§èƒ½ã®ç¶™ç¶šç›£è¦–",
                "ã‚³ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡",
                "çµ±è¨ˆçš„æœ‰æ„æ€§ã®å®šæœŸæ¤œè¨¼"
            ],
            'future_enhancements': [
                "ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹æœ€é©åŒ–é«˜é€ŸåŒ–",
                "ã‚¹ãƒªãƒƒãƒ‘ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ ",
                "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ "
            ]
        }
        
        print("\nğŸš€ æ¨å¥¨äº‹é …:")
        print("\nâš¡ å³åº§å®Ÿè¡Œ:")
        for action in recommendations['immediate_actions']:
            print(f"  â€¢ {action}")
        
        print("\nğŸ‘ï¸ ç›£è¦–è¦ä»¶:")
        for req in recommendations['monitoring_requirements']:
            print(f"  â€¢ {req}")
        
        print("\nğŸ”® å°†æ¥æ‹¡å¼µ:")
        for enhancement in recommendations['future_enhancements']:
            print(f"  â€¢ {enhancement}")
        
        return recommendations
    
    def save_comparison_report(self, filename=None):
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"system_comparison_report_{timestamp}.json"
        
        try:
            report = self.generate_comprehensive_report()
            
            with open(filename, 'w') as f:
                json.dump(report, f, indent=2, default=str)
            
            print(f"\nâœ… æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {filename}")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒåˆ†æé–‹å§‹")
    print("=" * 60)
    
    # æ¯”è¼ƒåˆ†æåˆæœŸåŒ–
    comparison = SystemPerformanceComparison()
    
    # çµæœãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š
    old_results_path = "vectorbt_wfa_results_20250716_235344.json"
    corrected_results_path = "corrected_wfa_results_20250717_062858.json"
    
    # çµæœèª­ã¿è¾¼ã¿
    if not comparison.load_results(old_results_path, corrected_results_path):
        print("âŒ çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # åŒ…æ‹¬çš„æ¯”è¼ƒåˆ†æå®Ÿè¡Œ
    report = comparison.generate_comprehensive_report()
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    comparison.save_comparison_report()
    
    print("\nğŸ‰ ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½æ¯”è¼ƒåˆ†æå®Œäº†")

if __name__ == "__main__":
    main()