#!/usr/bin/env python3
"""
MT5ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  - ç•°å¸¸çµæœåŸå› ç©¶æ˜ãƒ»æ­£ç¢ºè§£æ±º
ç›®çš„: 95707%åç›Šç‡ç­‰ã®ç•°å¸¸å€¤ã®æ ¹æœ¬åŸå› ç‰¹å®šãƒ»ä¿®æ­£

æ¤œè¨¼é …ç›®:
1. exit_price=0.0ã®åŸå› 
2. æ™‚ç³»åˆ—çŸ›ç›¾ã®åŸå› 
3. ç•°å¸¸pip_profitè¨ˆç®—ã®åŸå› 
4. å®Ÿéš›ã®MT5ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨ã®é½Ÿé½¬

ä½œæˆè€…: Claudeï¼ˆç•°å¸¸å€¤æ ¹æœ¬è§£æ±ºæ‹…å½“ï¼‰
"""

import json
import logging
from datetime import datetime
from typing import Dict

import pandas as pd


class MT5DataVerificationSystem:
    """MT5ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»ç•°å¸¸å€¤åŸå› ç©¶æ˜ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, excel_path: str):
        self.excel_path = excel_path
        self.raw_data = None
        self.trades_df = None

        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
        )
        self.logger = logging.getLogger(__name__)

    def load_raw_data_for_inspection(self) -> bool:
        """ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»è©³ç´°æ¤œæŸ»ç”¨"""
        self.logger.info("=== ç”Ÿãƒ‡ãƒ¼ã‚¿è©³ç´°æ¤œæŸ»èª­ã¿è¾¼ã¿ ===")

        try:
            # å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self.raw_data = pd.read_excel(self.excel_path, header=None)
            self.logger.info(f"å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {self.raw_data.shape}")

            # è¡Œ59å‘¨è¾ºã®è©³ç´°ç¢ºèª
            self.logger.info("=== è¡Œ59å‘¨è¾ºã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ç¢ºèª ===")
            for i in range(55, 65):
                if i < len(self.raw_data):
                    row_data = self.raw_data.iloc[i].values
                    self.logger.info(f"è¡Œ{i}: {row_data}")

            # ãƒ˜ãƒƒãƒ€ãƒ¼ç¢ºå®šãƒ»ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            header_row = 59
            data_start_row = 60

            header = self.raw_data.iloc[header_row].values
            data_rows = self.raw_data.iloc[data_start_row:].values

            self.trades_df = pd.DataFrame(data_rows, columns=header)
            self.trades_df = self.trades_df.dropna(how="all")

            self.logger.info(f"å–å¼•ãƒ‡ãƒ¼ã‚¿æŠ½å‡º: {len(self.trades_df)}ä»¶")
            self.logger.info(f"åˆ—å: {list(self.trades_df.columns)}")

            return True

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            traceback.print_exc()
            return False

    def inspect_data_quality_issues(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œè©³ç´°æ¤œæŸ»"""
        self.logger.info("=== ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œè©³ç´°æ¤œæŸ» ===")

        issues = {
            "zero_prices": [],
            "time_inconsistencies": [],
            "missing_data_patterns": [],
            "column_data_types": {},
            "sample_data_inspection": {},
        }

        # åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©
        time_col = self.trades_df.columns[0]  # ç´„å®šæ™‚åˆ»
        order_col = self.trades_df.columns[1]  # æ³¨æ–‡
        price_col = self.trades_df.columns[6]  # ä¾¡æ ¼
        comment_col = self.trades_df.columns[12]  # ã‚³ãƒ¡ãƒ³ãƒˆ

        # 1. ã‚¼ãƒ­ä¾¡æ ¼èª¿æŸ»
        self.logger.info("--- ã‚¼ãƒ­ä¾¡æ ¼å•é¡Œèª¿æŸ» ---")
        zero_price_rows = self.trades_df[
            (pd.to_numeric(self.trades_df[price_col], errors="coerce") == 0)
            | (self.trades_df[price_col].isna())
        ]

        self.logger.info(f"ã‚¼ãƒ­/NULLä¾¡æ ¼ä»¶æ•°: {len(zero_price_rows)}")

        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        for i, (idx, row) in enumerate(zero_price_rows.head(10).iterrows()):
            issues["zero_prices"].append(
                {
                    "row_index": idx,
                    "order_id": row[order_col],
                    "time": row[time_col],
                    "price": row[price_col],
                    "comment": row[comment_col],
                }
            )
            self.logger.info(
                f"ã‚¼ãƒ­ä¾¡æ ¼ä¾‹{i+1}: æ³¨æ–‡{row[order_col]}, æ™‚åˆ»{row[time_col]}, ä¾¡æ ¼{row[price_col]}, ã‚³ãƒ¡ãƒ³ãƒˆ{row[comment_col]}"
            )

        # 2. æ™‚ç³»åˆ—çŸ›ç›¾èª¿æŸ»
        self.logger.info("--- æ™‚ç³»åˆ—çŸ›ç›¾èª¿æŸ» ---")

        # position_idã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦æ™‚ç³»åˆ—ãƒã‚§ãƒƒã‚¯
        position_groups = self.trades_df.groupby(order_col)
        time_issues = 0

        for position_id, group in list(position_groups)[:100]:  # æœ€åˆã®100ã‚°ãƒ«ãƒ¼ãƒ—
            if len(group) >= 2:
                # æ™‚é–“ã‚’ datetime ã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
                group_with_time = group.copy()
                try:
                    group_with_time["parsed_time"] = pd.to_datetime(
                        group_with_time[time_col], format="%Y.%m.%d %H:%M:%S"
                    )
                    sorted_group = group_with_time.sort_values("parsed_time")

                    # JamesORBã‚¨ãƒ³ãƒˆãƒªãƒ¼ã¨æ±ºæ¸ˆã®æ™‚ç³»åˆ—ãƒã‚§ãƒƒã‚¯
                    entry_rows = sorted_group[
                        sorted_group[comment_col]
                        .astype(str)
                        .str.contains("JamesORB", na=False)
                    ]
                    exit_rows = sorted_group[
                        sorted_group[comment_col]
                        .astype(str)
                        .str.contains("sl |tp ", na=False)
                    ]

                    if len(entry_rows) > 0 and len(exit_rows) > 0:
                        entry_time = entry_rows.iloc[0]["parsed_time"]
                        exit_time = exit_rows.iloc[-1]["parsed_time"]

                        if exit_time < entry_time:
                            time_issues += 1
                            issues["time_inconsistencies"].append(
                                {
                                    "position_id": position_id,
                                    "entry_time": str(entry_time),
                                    "exit_time": str(exit_time),
                                    "time_diff_minutes": (
                                        entry_time - exit_time
                                    ).total_seconds()
                                    / 60,
                                }
                            )

                            if time_issues <= 5:  # æœ€åˆã®5ä»¶è¡¨ç¤º
                                self.logger.info(
                                    f"æ™‚ç³»åˆ—çŸ›ç›¾: position_id={position_id}, entry={entry_time}, exit={exit_time}"
                                )

                except Exception:
                    continue

        self.logger.info(f"æ™‚ç³»åˆ—çŸ›ç›¾ä»¶æ•°: {time_issues}")

        # 3. ãƒ‡ãƒ¼ã‚¿å‹ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        self.logger.info("--- ãƒ‡ãƒ¼ã‚¿å‹ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ ---")

        for i, col in enumerate(self.trades_df.columns):
            if i > 12 or pd.isna(col):  # ä¸»è¦åˆ—ã®ã¿ã€NaNåˆ—ã‚¹ã‚­ãƒƒãƒ—
                continue

            try:
                sample_series = self.trades_df[col].dropna().head(20)
                sample_data = sample_series.values.tolist()
                unique_types = {
                    type(val).__name__ for val in sample_data if pd.notna(val)
                }
            except Exception as e:
                sample_data = []
                unique_types = []
                self.logger.warning(f"åˆ—{i}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

            issues["column_data_types"][f"col_{i}_{col}"] = {
                "unique_types": list(unique_types),
                "sample_values": [str(val) for val in sample_data[:5]],
                "null_count": self.trades_df[col].isna().sum(),
                "total_count": len(self.trades_df),
            }

        # 4. ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒèª¿æŸ»
        self.logger.info("--- ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒèª¿æŸ» ---")

        price_numeric = pd.to_numeric(self.trades_df[price_col], errors="coerce")
        price_stats = {
            "min": price_numeric.min(),
            "max": price_numeric.max(),
            "mean": price_numeric.mean(),
            "std": price_numeric.std(),
            "zero_count": (price_numeric == 0).sum(),
            "null_count": price_numeric.isna().sum(),
            "valid_count": price_numeric.notna().sum(),
        }

        issues["price_distribution"] = price_stats

        self.logger.info(
            f"ä¾¡æ ¼çµ±è¨ˆ: min={price_stats['min']}, max={price_stats['max']}, mean={price_stats['mean']:.5f}"
        )
        self.logger.info(
            f"ã‚¼ãƒ­ä¾¡æ ¼: {price_stats['zero_count']}, NULL: {price_stats['null_count']}, æœ‰åŠ¹: {price_stats['valid_count']}"
        )

        return issues

    def analyze_specific_problematic_trades(self) -> Dict:
        """å…·ä½“çš„å•é¡Œå–å¼•ã®è©³ç´°åˆ†æ"""
        self.logger.info("=== å•é¡Œå–å¼•è©³ç´°åˆ†æ ===")

        analysis = {
            "zero_exit_price_trades": [],
            "extreme_profit_trades": [],
            "time_reversal_trades": [],
        }

        # åˆ—å®šç¾©
        order_col = self.trades_df.columns[1]  # æ³¨æ–‡
        time_col = self.trades_df.columns[0]  # ç´„å®šæ™‚åˆ»
        price_col = self.trades_df.columns[6]  # ä¾¡æ ¼
        comment_col = self.trades_df.columns[12]  # ã‚³ãƒ¡ãƒ³ãƒˆ

        # å•é¡Œã®ã‚ã‚‹position_idã‚’ç‰¹å®š
        position_groups = self.trades_df.groupby(order_col)

        for position_id, group in list(position_groups)[:1000]:  # æœ€åˆã®1000ã‚°ãƒ«ãƒ¼ãƒ—
            if len(group) < 2:
                continue

            # JamesORBã‚¨ãƒ³ãƒˆãƒªãƒ¼ã¨æ±ºæ¸ˆã‚’æŠ½å‡º
            entry_rows = group[
                group[comment_col].astype(str).str.contains("JamesORB", na=False)
            ]
            exit_rows = group[
                group[comment_col].astype(str).str.contains("sl |tp ", na=False)
            ]

            if len(entry_rows) == 0 or len(exit_rows) == 0:
                continue

            entry = entry_rows.iloc[0]
            exit_trade = exit_rows.iloc[-1]

            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å¤‰æ›
            try:
                entry_price = (
                    float(entry[price_col]) if pd.notna(entry[price_col]) else 0.0
                )
                exit_price = (
                    float(exit_trade[price_col])
                    if pd.notna(exit_trade[price_col])
                    else 0.0
                )
            except:
                entry_price = 0.0
                exit_price = 0.0

            # 1. ã‚¼ãƒ­æ±ºæ¸ˆä¾¡æ ¼ã®å–å¼•
            if exit_price == 0.0:
                analysis["zero_exit_price_trades"].append(
                    {
                        "position_id": position_id,
                        "entry_time": str(entry[time_col]),
                        "exit_time": str(exit_trade[time_col]),
                        "entry_price": entry_price,
                        "exit_price": exit_price,
                        "entry_comment": str(entry[comment_col]),
                        "exit_comment": str(exit_trade[comment_col]),
                        "group_size": len(group),
                    }
                )

            # 2. ç•°å¸¸åˆ©ç›Šã®å–å¼•ï¼ˆ1000pipä»¥ä¸Šï¼‰
            if entry_price > 0 and exit_price > 0:
                pip_diff = abs(entry_price - exit_price) * 10000
                if pip_diff > 1000:  # 1000pipä»¥ä¸Š
                    analysis["extreme_profit_trades"].append(
                        {
                            "position_id": position_id,
                            "entry_price": entry_price,
                            "exit_price": exit_price,
                            "pip_difference": pip_diff,
                            "entry_comment": str(entry[comment_col]),
                            "exit_comment": str(exit_trade[comment_col]),
                        }
                    )

            # 3. æ™‚ç³»åˆ—é€†è»¢å–å¼•
            try:
                entry_time = pd.to_datetime(entry[time_col], format="%Y.%m.%d %H:%M:%S")
                exit_time = pd.to_datetime(
                    exit_trade[time_col], format="%Y.%m.%d %H:%M:%S"
                )

                if exit_time < entry_time:
                    analysis["time_reversal_trades"].append(
                        {
                            "position_id": position_id,
                            "entry_time": str(entry_time),
                            "exit_time": str(exit_time),
                            "time_diff_hours": (entry_time - exit_time).total_seconds()
                            / 3600,
                        }
                    )
            except:
                continue

        # çµæœã‚µãƒãƒªãƒ¼
        self.logger.info(f"ã‚¼ãƒ­æ±ºæ¸ˆä¾¡æ ¼å–å¼•: {len(analysis['zero_exit_price_trades'])}ä»¶")
        self.logger.info(f"ç•°å¸¸åˆ©ç›Šå–å¼•(>1000pip): {len(analysis['extreme_profit_trades'])}ä»¶")
        self.logger.info(f"æ™‚ç³»åˆ—é€†è»¢å–å¼•: {len(analysis['time_reversal_trades'])}ä»¶")

        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        for category, trades in analysis.items():
            if trades and len(trades) > 0:
                self.logger.info(f"\n{category} ã‚µãƒ³ãƒ—ãƒ«:")
                for i, trade in enumerate(trades[:3]):
                    self.logger.info(f"  {i+1}: {trade}")

        return analysis

    def identify_root_cause_and_solution(
        self, quality_issues: Dict, problem_trades: Dict
    ) -> Dict:
        """æ ¹æœ¬åŸå› ç‰¹å®šãƒ»è§£æ±ºç­–ææ¡ˆ"""
        self.logger.info("=== æ ¹æœ¬åŸå› ç‰¹å®šãƒ»è§£æ±ºç­–ææ¡ˆ ===")

        root_causes = {
            "primary_issues": [],
            "data_structure_problems": [],
            "calculation_logic_errors": [],
            "solutions": [],
        }

        # 1. ä¸»è¦å•é¡Œç‰¹å®š
        zero_exit_count = len(problem_trades["zero_exit_price_trades"])
        time_reversal_count = len(problem_trades["time_reversal_trades"])
        extreme_profit_count = len(problem_trades["extreme_profit_trades"])

        if zero_exit_count > 100:
            root_causes["primary_issues"].append(
                {
                    "issue": "massive_zero_exit_prices",
                    "count": zero_exit_count,
                    "severity": "CRITICAL",
                    "description": "å¤§é‡ã®ã‚¼ãƒ­æ±ºæ¸ˆä¾¡æ ¼ - ãƒ‡ãƒ¼ã‚¿æ§‹é€ ç†è§£ä¸å®Œå…¨",
                }
            )

        if time_reversal_count > 50:
            root_causes["primary_issues"].append(
                {
                    "issue": "time_sequence_errors",
                    "count": time_reversal_count,
                    "severity": "HIGH",
                    "description": "æ™‚ç³»åˆ—é€†è»¢ - ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ãƒˆãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å•é¡Œ",
                }
            )

        # 2. ãƒ‡ãƒ¼ã‚¿æ§‹é€ å•é¡Œåˆ†æ
        price_stats = quality_issues.get("price_distribution", {})
        zero_price_ratio = price_stats.get("zero_count", 0) / price_stats.get(
            "total_count", 1
        )

        if zero_price_ratio > 0.1:  # 10%ä»¥ä¸ŠãŒã‚¼ãƒ­ä¾¡æ ¼
            root_causes["data_structure_problems"].append(
                {
                    "problem": "incorrect_price_column_identification",
                    "ratio": zero_price_ratio,
                    "description": "ä¾¡æ ¼åˆ—ã®èª¤ç‰¹å®š - åˆ—6ãŒå®Ÿéš›ã®ä¾¡æ ¼åˆ—ã§ãªã„å¯èƒ½æ€§",
                }
            )

        # 3. è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ç‰¹å®š
        if extreme_profit_count > 0:
            root_causes["calculation_logic_errors"].append(
                {
                    "error": "pip_calculation_magnitude_error",
                    "count": extreme_profit_count,
                    "description": "pipè¨ˆç®—ã®æ¡æ•°ã‚¨ãƒ©ãƒ¼ - 10000å€ã®é‡è¤‡é©ç”¨ç–‘ã„",
                }
            )

        # 4. è§£æ±ºç­–ææ¡ˆ
        root_causes["solutions"] = [
            {
                "priority": 1,
                "solution": "correct_price_column_identification",
                "description": "æ­£ç¢ºãªä¾¡æ ¼åˆ—ç‰¹å®š - æ‰‹å‹•ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç¢ºèª",
                "implementation": "å®Ÿéš›ã®å–å¼•ãƒ¬ã‚³ãƒ¼ãƒ‰10-20ä»¶ã‚’æ‰‹å‹•ç¢ºèªã—ã€ã©ã®åˆ—ãŒæ­£ç¢ºãªä¾¡æ ¼ã‹ç‰¹å®š",
            },
            {
                "priority": 2,
                "solution": "fix_time_sequence_logic",
                "description": "æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆä¿®æ­£",
                "implementation": "ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºæ™‚ã®æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆãƒ­ã‚¸ãƒƒã‚¯è¦‹ç›´ã—ãƒ»ä¿®æ­£",
            },
            {
                "priority": 3,
                "solution": "recalibrate_pip_calculation",
                "description": "pipè¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯å†èª¿æ•´",
                "implementation": "EURUSDæ¨™æº–pipè¨ˆç®— (price_diff * 10000) ã®æ¤œè¨¼ãƒ»ä¿®æ­£",
            },
            {
                "priority": 4,
                "solution": "implement_data_validation",
                "description": "ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ",
                "implementation": "ç•°å¸¸å€¤æ¤œå‡ºãƒ»é™¤å¤–ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…",
            },
        ]

        return root_causes

    def run_complete_verification(self) -> Dict:
        """å®Œå…¨æ¤œè¨¼å®Ÿè¡Œ"""
        self.logger.info("ğŸ” === MT5ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ ===")

        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if not self.load_raw_data_for_inspection():
            return {"error": "Failed to load data"}

        # 2. å“è³ªå•é¡Œæ¤œæŸ»
        quality_issues = self.inspect_data_quality_issues()

        # 3. å•é¡Œå–å¼•åˆ†æ
        problem_trades = self.analyze_specific_problematic_trades()

        # 4. æ ¹æœ¬åŸå› ç‰¹å®š
        root_analysis = self.identify_root_cause_and_solution(
            quality_issues, problem_trades
        )

        # 5. æ¤œè¨¼çµæœçµ±åˆ
        verification_result = {
            "timestamp": datetime.now().isoformat(),
            "verification_method": "comprehensive_data_quality_inspection",
            "data_quality_issues": quality_issues,
            "problematic_trades_analysis": problem_trades,
            "root_cause_analysis": root_analysis,
            "summary": {
                "critical_issues_found": len(
                    [
                        issue
                        for issue in root_analysis["primary_issues"]
                        if issue["severity"] == "CRITICAL"
                    ]
                ),
                "total_problems_identified": len(root_analysis["primary_issues"])
                + len(root_analysis["data_structure_problems"])
                + len(root_analysis["calculation_logic_errors"]),
                "recommended_actions": len(root_analysis["solutions"]),
            },
        }

        # 6. çµæœä¿å­˜
        output_path = "/home/trader/Trading-Development/2.ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ‰‹æ³•ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/MT5_Results/data_verification_report.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(verification_result, f, indent=2, ensure_ascii=False, default=str)

        self.logger.info(f"âœ… æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")
        self.logger.info(
            f"ğŸš¨ é‡å¤§å•é¡Œ: {verification_result['summary']['critical_issues_found']}ä»¶"
        )
        self.logger.info(
            f"ğŸ“‹ è§£æ±ºç­–: {verification_result['summary']['recommended_actions']}é …ç›®"
        )

        return verification_result


def main():
    """æ¤œè¨¼å®Ÿè¡Œãƒ¡ã‚¤ãƒ³"""
    excel_path = "/home/trader/Trading-Development/2.ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ‰‹æ³•ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/MT5_Results/Reportãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ-900179988.xlsx"

    verifier = MT5DataVerificationSystem(excel_path)
    result = verifier.run_complete_verification()

    return result


if __name__ == "__main__":
    main()
