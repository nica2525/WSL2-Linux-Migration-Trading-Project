#!/usr/bin/env python3
"""
MT5ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬ç²¾å¯†æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
ç›®çš„: 14.6æ™‚é–“ã®æ™‚é–“å·®ãŒã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³èµ·å› ã‹ã®å¾¹åº•æ¤œè¨¼

æ¤œè¨¼é …ç›®:
1. æ™‚é–“å·®ã®åˆ†å¸ƒçµ±è¨ˆè§£æ
2. æ¨™æº–ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å·®ã¨ã®ç…§åˆ
3. å­£ç¯€å¤‰å‹•ï¼ˆã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ ï¼‰å½±éŸ¿åˆ†æ
4. åœ°ç†çš„æ™‚å·®ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
5. MT5ã‚µãƒ¼ãƒãƒ¼æ™‚é–“è¨­å®šæ¤œè¨¼

ä½œæˆè€…: Claudeï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬å°‚é–€æ¤œè¨¼æ‹…å½“ï¼‰
"""

import json
import logging
from datetime import datetime
from typing import Dict

import numpy as np
import pandas as pd


class MT5TimezoneHypothesisVerification:
    """MT5ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬ç²¾å¯†æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, excel_path: str):
        self.excel_path = excel_path
        self.raw_data = None
        self.trades_df = None

        logging.basicConfig(
            level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
        )
        self.logger = logging.getLogger(__name__)

        # ä¸–ç•Œä¸»è¦ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å·®ï¼ˆUTCåŸºæº–ï¼‰
        self.major_timezones = {
            "UTC+0": 0,
            "London": 0,  # GMT
            "Frankfurt": 1,  # CET
            "Moscow": 3,  # MSK
            "Dubai": 4,  # GST
            "Tokyo": 9,  # JST
            "Sydney": 10,  # AEST
            "New_York": -5,  # EST
            "Chicago": -6,  # CST
            "Los_Angeles": -8,  # PST
        }

        # MT5ä¸»è¦ã‚µãƒ¼ãƒãƒ¼ã¨ãã®æ™‚å·®
        self.mt5_server_timezones = {
            "Alpari": 2,  # EET
            "FXDD": -5,  # EST
            "FXTM": 2,  # EET
            "XM": 2,  # EET
            "IC_Markets": 2,  # EET
            "Pepperstone": 2,  # EET
            "OANDA": -5,  # EST (æ¨å®š)
        }

    def load_data_for_timezone_analysis(self) -> bool:
        """ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            self.raw_data = pd.read_excel(self.excel_path, header=None)

            header_row = 59
            data_start_row = 60

            header = self.raw_data.iloc[header_row].values
            data_rows = self.raw_data.iloc[data_start_row:].values

            self.trades_df = pd.DataFrame(data_rows, columns=header)
            self.trades_df = self.trades_df.dropna(how="all")

            self.logger.info(f"ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³åˆ†æãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.trades_df)}ä»¶")
            return True

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def analyze_time_difference_distribution(self) -> Dict:
        """æ™‚é–“å·®åˆ†å¸ƒçµ±è¨ˆè§£æ"""
        self.logger.info("=== æ™‚é–“å·®åˆ†å¸ƒçµ±è¨ˆè§£æ ===")

        analysis = {
            "time_differences": [],
            "statistical_summary": {},
            "distribution_patterns": {},
            "timezone_correlation": {},
        }

        # åˆ—å®šç¾©
        time_col = self.trades_df.columns[0]
        order_col = self.trades_df.columns[1]
        comment_col = self.trades_df.columns[12]

        # position_idã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€æ™‚é–“å·®ã‚’åé›†
        position_groups = self.trades_df.groupby(order_col)

        time_diffs_hours = []
        time_diffs_minutes = []
        reversal_cases = []

        sample_count = 0
        for position_id, group in position_groups:
            if sample_count >= 2000:  # 2000ã‚µãƒ³ãƒ—ãƒ«ã§è©³ç´°åˆ†æ
                break

            if len(group) < 2:
                continue

            sample_count += 1

            # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã¨æ±ºæ¸ˆã‚’æŠ½å‡º
            entry_rows = group[
                group[comment_col].astype(str).str.contains("JamesORB", na=False)
            ]
            exit_rows = group[
                group[comment_col].astype(str).str.contains("sl |tp ", na=False)
            ]

            if len(entry_rows) == 0 or len(exit_rows) == 0:
                continue

            try:
                entry_time = pd.to_datetime(
                    entry_rows.iloc[0][time_col], format="%Y.%m.%d %H:%M:%S"
                )
                exit_time = pd.to_datetime(
                    exit_rows.iloc[0][time_col], format="%Y.%m.%d %H:%M:%S"
                )

                # æ™‚é–“å·®è¨ˆç®—
                time_diff = entry_time - exit_time
                total_seconds = time_diff.total_seconds()

                if total_seconds > 0:  # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ > ã‚¨ã‚°ã‚¸ãƒƒãƒˆï¼ˆé€†è»¢ï¼‰
                    hours_diff = total_seconds / 3600
                    minutes_diff = total_seconds / 60

                    time_diffs_hours.append(hours_diff)
                    time_diffs_minutes.append(minutes_diff)

                    reversal_cases.append(
                        {
                            "position_id": position_id,
                            "entry_time": entry_time,
                            "exit_time": exit_time,
                            "hours_diff": hours_diff,
                            "minutes_diff": minutes_diff,
                            "exit_comment": str(exit_rows.iloc[0][comment_col]),
                        }
                    )

            except Exception:
                continue

        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        if time_diffs_hours:
            analysis["statistical_summary"] = {
                "sample_size": len(time_diffs_hours),
                "mean_hours": float(np.mean(time_diffs_hours)),
                "median_hours": float(np.median(time_diffs_hours)),
                "std_hours": float(np.std(time_diffs_hours)),
                "min_hours": float(np.min(time_diffs_hours)),
                "max_hours": float(np.max(time_diffs_hours)),
                "percentiles": {
                    "25th": float(np.percentile(time_diffs_hours, 25)),
                    "75th": float(np.percentile(time_diffs_hours, 75)),
                    "90th": float(np.percentile(time_diffs_hours, 90)),
                    "95th": float(np.percentile(time_diffs_hours, 95)),
                },
            }

            # åˆ†å¸ƒãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            # 1æ™‚é–“åˆ»ã¿ã§ã®åˆ†å¸ƒ
            hour_bins = np.arange(0, max(time_diffs_hours) + 1, 1)
            hour_distribution, _ = np.histogram(time_diffs_hours, bins=hour_bins)

            analysis["distribution_patterns"] = {
                "hourly_distribution": {
                    f"{int(hour_bins[i])}-{int(hour_bins[i+1])}h": int(count)
                    for i, count in enumerate(hour_distribution)
                },
                "peak_hours": [
                    int(hour_bins[i])
                    for i, count in enumerate(hour_distribution)
                    if count == max(hour_distribution)
                ],
            }

        analysis["time_differences"] = reversal_cases[:100]  # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜

        self.logger.info(f"æ™‚é–“å·®ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(time_diffs_hours)}")
        if time_diffs_hours:
            self.logger.info(f"å¹³å‡æ™‚é–“å·®: {np.mean(time_diffs_hours):.2f}æ™‚é–“")
            self.logger.info(f"ä¸­å¤®å€¤: {np.median(time_diffs_hours):.2f}æ™‚é–“")
            self.logger.info(f"æ¨™æº–åå·®: {np.std(time_diffs_hours):.2f}æ™‚é–“")

        return analysis

    def verify_standard_timezone_correlation(self, time_analysis: Dict) -> Dict:
        """æ¨™æº–ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å·®ã¨ã®ç›¸é–¢æ¤œè¨¼"""
        self.logger.info("=== æ¨™æº–ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å·®ç›¸é–¢æ¤œè¨¼ ===")

        if not time_analysis["statistical_summary"]:
            return {"error": "No time difference data available"}

        mean_diff = time_analysis["statistical_summary"]["mean_hours"]
        std_diff = time_analysis["statistical_summary"]["std_hours"]

        correlation_analysis = {
            "timezone_matches": [],
            "best_matches": [],
            "seasonal_variation_test": {},
            "server_timezone_hypothesis": {},
        }

        # ä¸»è¦ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å·®ã¨ã®æ¯”è¼ƒ
        for tz_name, tz_offset in self.major_timezones.items():
            # æ—¥æœ¬æ™‚é–“ï¼ˆJST = UTC+9ï¼‰ã¨ã®å·®ã‚’è¨ˆç®—
            jst_offset = 9
            expected_diff = abs(jst_offset - tz_offset)

            # ã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ è€ƒæ…®ï¼ˆÂ±1æ™‚é–“ï¼‰
            matches = []
            for seasonal_adjust in [0, 1, -1]:  # æ¨™æº–æ™‚ã€ã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ ã€é€†ã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ 
                adjusted_diff = expected_diff + seasonal_adjust

                if abs(mean_diff - adjusted_diff) <= 2 * std_diff:  # 2Ïƒå†…ãªã‚‰ä¸€è‡´
                    confidence = max(
                        0, 1 - abs(mean_diff - adjusted_diff) / (2 * std_diff)
                    )
                    matches.append(
                        {
                            "timezone": tz_name,
                            "expected_diff": adjusted_diff,
                            "actual_diff": mean_diff,
                            "difference": abs(mean_diff - adjusted_diff),
                            "confidence": confidence,
                            "seasonal_adjustment": seasonal_adjust,
                        }
                    )

            if matches:
                correlation_analysis["timezone_matches"].extend(matches)

        # MT5ã‚µãƒ¼ãƒãƒ¼ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã¨ã®æ¯”è¼ƒ
        for server_name, server_tz in self.mt5_server_timezones.items():
            jst_offset = 9
            expected_diff = abs(jst_offset - server_tz)

            if abs(mean_diff - expected_diff) <= 2 * std_diff:
                correlation_analysis["server_timezone_hypothesis"][server_name] = {
                    "expected_diff": expected_diff,
                    "actual_diff": mean_diff,
                    "match_probability": max(
                        0, 1 - abs(mean_diff - expected_diff) / (2 * std_diff)
                    ),
                }

        # ãƒ™ã‚¹ãƒˆãƒãƒƒãƒã‚’ç‰¹å®š
        all_matches = correlation_analysis["timezone_matches"]
        if all_matches:
            correlation_analysis["best_matches"] = sorted(
                all_matches, key=lambda x: x["difference"]
            )[:5]

        return correlation_analysis

    def analyze_seasonal_patterns(self, time_analysis: Dict) -> Dict:
        """å­£ç¯€å¤‰å‹•ï¼ˆã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        self.logger.info("=== å­£ç¯€å¤‰å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ ===")

        seasonal_analysis = {
            "monthly_patterns": {},
            "daylight_saving_evidence": {},
            "seasonal_consistency": {},
        }

        # æœˆåˆ¥æ™‚é–“å·®åˆ†æ
        if "time_differences" in time_analysis:
            monthly_diffs = {}

            for case in time_analysis["time_differences"]:
                try:
                    entry_time = pd.to_datetime(case["entry_time"])
                    month = entry_time.month

                    if month not in monthly_diffs:
                        monthly_diffs[month] = []

                    monthly_diffs[month].append(case["hours_diff"])
                except:
                    continue

            # æœˆåˆ¥çµ±è¨ˆ
            for month, diffs in monthly_diffs.items():
                if len(diffs) >= 5:  # æœ€ä½5ã‚µãƒ³ãƒ—ãƒ«
                    seasonal_analysis["monthly_patterns"][f"month_{month}"] = {
                        "sample_size": len(diffs),
                        "mean_hours": float(np.mean(diffs)),
                        "std_hours": float(np.std(diffs)),
                        "median_hours": float(np.median(diffs)),
                    }

            # ã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ æœŸé–“ã®æ¤œè¨¼
            # ä¸€èˆ¬çš„ãªã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ æœŸé–“: 3æœˆæœ€çµ‚æ—¥æ›œæ—¥ - 10æœˆæœ€çµ‚æ—¥æ›œæ—¥ï¼ˆãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ï¼‰
            summer_months = [4, 5, 6, 7, 8, 9]  # 4-9æœˆã‚’ã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ æœŸé–“ã¨ã—ã¦ä»®å®š
            winter_months = [1, 2, 3, 10, 11, 12]

            summer_diffs = []
            winter_diffs = []

            for month, stats in seasonal_analysis["monthly_patterns"].items():
                month_num = int(month.split("_")[1])
                if month_num in summer_months:
                    summer_diffs.append(stats["mean_hours"])
                elif month_num in winter_months:
                    winter_diffs.append(stats["mean_hours"])

            if summer_diffs and winter_diffs:
                seasonal_analysis["daylight_saving_evidence"] = {
                    "summer_avg_hours": float(np.mean(summer_diffs)),
                    "winter_avg_hours": float(np.mean(winter_diffs)),
                    "seasonal_difference": float(
                        np.mean(summer_diffs) - np.mean(winter_diffs)
                    ),
                    "dst_hypothesis": "SUPPORTED"
                    if abs(np.mean(summer_diffs) - np.mean(winter_diffs)) >= 0.8
                    else "NOT_SUPPORTED",
                }

        return seasonal_analysis

    def verify_geographical_time_patterns(self, time_analysis: Dict) -> Dict:
        """åœ°ç†çš„æ™‚å·®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼"""
        self.logger.info("=== åœ°ç†çš„æ™‚å·®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼ ===")

        geo_analysis = {
            "market_session_analysis": {},
            "trading_hours_correlation": {},
            "regional_patterns": {},
        }

        # ä¸»è¦å¸‚å ´ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ã¨ã®ç›¸é–¢åˆ†æ
        market_sessions = {
            "Tokyo": {"start": 0, "end": 9, "timezone_offset": 0},  # JSTåŸºæº–
            "London": {"start": 16, "end": 1, "timezone_offset": -9},  # GMT vs JST
            "New_York": {"start": 22, "end": 7, "timezone_offset": -14},  # EST vs JST
            "Sydney": {"start": 21, "end": 6, "timezone_offset": 1},  # AEST vs JST
        }

        if "time_differences" in time_analysis and time_analysis["time_differences"]:
            for session_name, session_info in market_sessions.items():
                session_time_diffs = []

                for case in time_analysis["time_differences"]:
                    try:
                        entry_time = pd.to_datetime(case["entry_time"])
                        entry_hour = entry_time.hour

                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“å†…ã‹ãƒã‚§ãƒƒã‚¯
                        session_start = session_info["start"]
                        session_end = session_info["end"]

                        in_session = False
                        if session_start <= session_end:
                            in_session = session_start <= entry_hour <= session_end
                        else:  # æ—¥ã‚’ã¾ãŸãã‚»ãƒƒã‚·ãƒ§ãƒ³
                            in_session = (
                                entry_hour >= session_start or entry_hour <= session_end
                            )

                        if in_session:
                            session_time_diffs.append(case["hours_diff"])

                    except:
                        continue

                if len(session_time_diffs) >= 3:
                    expected_tz_diff = abs(session_info["timezone_offset"])
                    actual_avg_diff = np.mean(session_time_diffs)

                    geo_analysis["market_session_analysis"][session_name] = {
                        "sample_size": len(session_time_diffs),
                        "avg_time_diff": float(actual_avg_diff),
                        "expected_tz_diff": expected_tz_diff,
                        "correlation_strength": max(
                            0,
                            1
                            - abs(actual_avg_diff - expected_tz_diff)
                            / max(actual_avg_diff, expected_tz_diff),
                        ),
                    }

        return geo_analysis

    def generate_timezone_hypothesis_verdict(
        self,
        time_analysis: Dict,
        tz_correlation: Dict,
        seasonal_analysis: Dict,
        geo_analysis: Dict,
    ) -> Dict:
        """ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬æœ€çµ‚åˆ¤å®š"""

        verdict = {
            "hypothesis_strength": 0.0,
            "supporting_evidence": [],
            "contradicting_evidence": [],
            "confidence_level": "LOW",
            "final_conclusion": "",
            "alternative_explanations": [],
        }

        evidence_scores = []

        # 1. æ¨™æº–ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å·®ã¨ã®ä¸€è‡´åº¦
        if "best_matches" in tz_correlation and tz_correlation["best_matches"]:
            best_match = tz_correlation["best_matches"][0]
            if best_match["confidence"] > 0.7:
                evidence_scores.append(0.8)
                verdict["supporting_evidence"].append(
                    f"æ¨™æº–ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³'{best_match['timezone']}'ã¨é«˜ã„ä¸€è‡´åº¦ ({best_match['confidence']:.2f})"
                )
            elif best_match["confidence"] > 0.4:
                evidence_scores.append(0.5)
                verdict["supporting_evidence"].append(
                    f"æ¨™æº–ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³'{best_match['timezone']}'ã¨ä¸­ç¨‹åº¦ã®ä¸€è‡´åº¦ ({best_match['confidence']:.2f})"
                )
            else:
                evidence_scores.append(0.2)
                verdict["contradicting_evidence"].append(
                    f"æ¨™æº–ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã¨ã®ä¸€è‡´åº¦ãŒä½ã„ ({best_match['confidence']:.2f})"
                )

        # 2. å­£ç¯€å¤‰å‹•ï¼ˆã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ ï¼‰ã®è¨¼æ‹ 
        if (
            "daylight_saving_evidence" in seasonal_analysis
            and seasonal_analysis["daylight_saving_evidence"]
        ):
            dst_evidence = seasonal_analysis["daylight_saving_evidence"]
            if dst_evidence.get("dst_hypothesis") == "SUPPORTED":
                evidence_scores.append(0.7)
                verdict["supporting_evidence"].append(
                    f"ã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ å¤‰å‹•ã‚’ç¢ºèª (å¤å†¬å·®: {dst_evidence['seasonal_difference']:.2f}æ™‚é–“)"
                )
            else:
                evidence_scores.append(0.3)
                verdict["contradicting_evidence"].append(
                    f"ã‚µãƒãƒ¼ã‚¿ã‚¤ãƒ å¤‰å‹•ãŒä¸æ˜ç¢º (å¤å†¬å·®: {dst_evidence.get('seasonal_difference', 0):.2f}æ™‚é–“)"
                )

        # 3. æ™‚é–“å·®ã®ä¸€è²«æ€§
        if "statistical_summary" in time_analysis:
            stats = time_analysis["statistical_summary"]
            cv = (
                stats["std_hours"] / stats["mean_hours"]
                if stats["mean_hours"] > 0
                else 1
            )

            if cv < 0.2:  # å¤‰å‹•ä¿‚æ•°20%æœªæº€ãªã‚‰ä¸€è²«æ€§é«˜
                evidence_scores.append(0.8)
                verdict["supporting_evidence"].append(f"æ™‚é–“å·®ã®ä¸€è²«æ€§ãŒé«˜ã„ (å¤‰å‹•ä¿‚æ•°: {cv:.2f})")
            elif cv < 0.5:
                evidence_scores.append(0.5)
                verdict["supporting_evidence"].append(f"æ™‚é–“å·®ã®ä¸€è²«æ€§ãŒä¸­ç¨‹åº¦ (å¤‰å‹•ä¿‚æ•°: {cv:.2f})")
            else:
                evidence_scores.append(0.2)
                verdict["contradicting_evidence"].append(
                    f"æ™‚é–“å·®ã®ã°ã‚‰ã¤ããŒå¤§ãã„ (å¤‰å‹•ä¿‚æ•°: {cv:.2f})"
                )

        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        if evidence_scores:
            verdict["hypothesis_strength"] = sum(evidence_scores) / len(evidence_scores)

        # ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«æ±ºå®š
        if verdict["hypothesis_strength"] >= 0.8:
            verdict["confidence_level"] = "VERY_HIGH"
            verdict["final_conclusion"] = "ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬ã¯æ¥µã‚ã¦å¦¥å½“"
        elif verdict["hypothesis_strength"] >= 0.6:
            verdict["confidence_level"] = "HIGH"
            verdict["final_conclusion"] = "ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬ã¯å¦¥å½“"
        elif verdict["hypothesis_strength"] >= 0.4:
            verdict["confidence_level"] = "MODERATE"
            verdict["final_conclusion"] = "ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬ã¯éƒ¨åˆ†çš„ã«å¦¥å½“"
        else:
            verdict["confidence_level"] = "LOW"
            verdict["final_conclusion"] = "ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬ã¯ç–‘ã‚ã—ã„"

        return verdict

    def run_comprehensive_timezone_verification(self) -> Dict:
        """åŒ…æ‹¬çš„ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬æ¤œè¨¼å®Ÿè¡Œ"""
        self.logger.info("ğŸ• === MT5ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬åŒ…æ‹¬æ¤œè¨¼é–‹å§‹ ===")

        if not self.load_data_for_timezone_analysis():
            return {"error": "Failed to load data"}

        # æ¤œè¨¼1: æ™‚é–“å·®åˆ†å¸ƒçµ±è¨ˆè§£æ
        time_analysis = self.analyze_time_difference_distribution()

        # æ¤œè¨¼2: æ¨™æº–ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å·®ç›¸é–¢æ¤œè¨¼
        tz_correlation = self.verify_standard_timezone_correlation(time_analysis)

        # æ¤œè¨¼3: å­£ç¯€å¤‰å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        seasonal_analysis = self.analyze_seasonal_patterns(time_analysis)

        # æ¤œè¨¼4: åœ°ç†çš„æ™‚å·®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼
        geo_analysis = self.verify_geographical_time_patterns(time_analysis)

        # æœ€çµ‚åˆ¤å®š
        final_verdict = self.generate_timezone_hypothesis_verdict(
            time_analysis, tz_correlation, seasonal_analysis, geo_analysis
        )

        # çµæœçµ±åˆ
        comprehensive_result = {
            "timestamp": datetime.now().isoformat(),
            "verification_method": "MT5_Timezone_Hypothesis_Comprehensive_Verification",
            "time_difference_analysis": time_analysis,
            "timezone_correlation_analysis": tz_correlation,
            "seasonal_pattern_analysis": seasonal_analysis,
            "geographical_pattern_analysis": geo_analysis,
            "final_verdict": final_verdict,
        }

        # çµæœä¿å­˜
        output_path = "/home/trader/Trading-Development/2.ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ‰‹æ³•ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/MT5_Results/timezone_hypothesis_verification.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(
                comprehensive_result, f, indent=2, ensure_ascii=False, default=str
            )

        self.logger.info(f"âœ… ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬æ¤œè¨¼çµæœä¿å­˜: {output_path}")
        self.logger.info(f"ğŸ¯ æœ€çµ‚çµè«–: {final_verdict['final_conclusion']}")
        self.logger.info(f"ğŸ“Š ä¿¡é ¼åº¦: {final_verdict['confidence_level']}")
        self.logger.info(f"ğŸ”¢ ä»®èª¬å¼·åº¦: {final_verdict['hypothesis_strength']:.3f}")

        return comprehensive_result


def main():
    """ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»®èª¬æ¤œè¨¼å®Ÿè¡Œ"""
    excel_path = "/home/trader/Trading-Development/2.ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ‰‹æ³•ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/MT5_Results/Reportãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ-900179988.xlsx"

    verifier = MT5TimezoneHypothesisVerification(excel_path)
    results = verifier.run_comprehensive_timezone_verification()

    return results


if __name__ == "__main__":
    main()
