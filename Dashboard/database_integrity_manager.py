"""
Database Integrity Manager - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
Phase 3ç§»è¡Œã®å®Œå…¨å®Ÿè£…ã¨ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ä¿è¨¼
"""

import sqlite3
import os
import logging
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import shutil

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseIntegrityManager:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = "dashboard.db"):
        self.db_path = db_path
        self.backup_dir = "database_backups"
        self.migration_log = []
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(self.backup_dir, exist_ok=True)
    
    def create_backup(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_filename = f"dashboard_backup_{timestamp}.db"
            backup_path = os.path.join(self.backup_dir, backup_filename)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ”ãƒ¼
            shutil.copy2(self.db_path, backup_path)
            
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
            return backup_path
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def verify_database_structure(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ ã®æ¤œè¨¼"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            
            structure_info = {
                'tables': {},
                'issues': [],
                'verification_time': datetime.now().isoformat()
            }
            
            # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ ç¢ºèª
            for table in tables:
                cursor.execute(f"PRAGMA table_info({table})")
                columns = cursor.fetchall()
                
                structure_info['tables'][table] = {
                    'columns': [{'name': col[1], 'type': col[2], 'notnull': col[3], 'default': col[4]} for col in columns],
                    'column_count': len(columns)
                }
            
            # Phase 3å¿…é ˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¢ºèª
            required_tables = ['account_history', 'position_details', 'statistics_cache', 'system_monitoring']
            missing_tables = [table for table in required_tables if table not in tables]
            
            if missing_tables:
                structure_info['issues'].append({
                    'type': 'missing_tables',
                    'tables': missing_tables,
                    'severity': 'high'
                })
            
            # account_historyã®Phase 3ã‚«ãƒ©ãƒ ç¢ºèª
            if 'account_history' in structure_info['tables']:
                account_columns = [col['name'] for col in structure_info['tables']['account_history']['columns']]
                required_columns = ['server_time', 'trade_allowed', 'trade_expert', 'margin_so_mode']
                missing_columns = [col for col in required_columns if col not in account_columns]
                
                if missing_columns:
                    structure_info['issues'].append({
                        'type': 'missing_columns',
                        'table': 'account_history',
                        'columns': missing_columns,
                        'severity': 'medium'
                    })
            
            conn.close()
            
            # æ¤œè¨¼çµæœ
            structure_info['status'] = 'healthy' if not structure_info['issues'] else 'needs_migration'
            
            return structure_info
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}
    
    def check_data_integrity(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æ¤œè¨¼"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            integrity_info = {
                'checks': {},
                'issues': [],
                'check_time': datetime.now().isoformat()
            }
            
            # 1. å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
            cursor.execute("PRAGMA foreign_key_check")
            fk_violations = cursor.fetchall()
            if fk_violations:
                integrity_info['issues'].append({
                    'type': 'foreign_key_violations',
                    'count': len(fk_violations),
                    'details': fk_violations[:5]  # æœ€åˆã®5ä»¶ã®ã¿
                })
            
            # 2. ãƒ‡ãƒ¼ã‚¿å‹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            tables_to_check = ['account_history', 'position_details', 'position_history']
            
            for table in tables_to_check:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    count = cursor.fetchone()[0]
                    integrity_info['checks'][table] = {'record_count': count}
                    
                    # NULLãƒã‚§ãƒƒã‚¯ï¼ˆå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
                    if table == 'account_history':
                        cursor.execute("SELECT COUNT(*) FROM account_history WHERE balance IS NULL OR equity IS NULL")
                        null_count = cursor.fetchone()[0]
                        if null_count > 0:
                            integrity_info['issues'].append({
                                'type': 'null_required_fields',
                                'table': table,
                                'count': null_count
                            })
                    
                except sqlite3.OperationalError as e:
                    integrity_info['issues'].append({
                        'type': 'table_access_error',
                        'table': table,
                        'error': str(e)
                    })
            
            # 3. é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
            duplicate_checks = [
                ("position_details", "ticket, timestamp"),
                ("statistics_cache", "calculation_date, period_type")
            ]
            
            for table, fields in duplicate_checks:
                try:
                    cursor.execute(f"""
                        SELECT {fields}, COUNT(*) as cnt 
                        FROM {table} 
                        GROUP BY {fields} 
                        HAVING COUNT(*) > 1
                    """)
                    duplicates = cursor.fetchall()
                    if duplicates:
                        integrity_info['issues'].append({
                            'type': 'duplicate_records',
                            'table': table,
                            'count': len(duplicates)
                        })
                except sqlite3.OperationalError:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    pass
            
            conn.close()
            
            # æ•´åˆæ€§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            integrity_info['status'] = 'healthy' if not integrity_info['issues'] else 'has_issues'
            
            return integrity_info
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}
    
    def perform_migration(self) -> Dict:
        """Phase 3ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ"""
        try:
            migration_result = {
                'started_at': datetime.now().isoformat(),
                'steps': [],
                'success': False,
                'backup_created': '',
                'errors': []
            }
            
            # 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_path = self.create_backup()
            migration_result['backup_created'] = backup_path
            
            if not backup_path:
                migration_result['errors'].append("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—")
                return migration_result
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # 2. Phase 3ã‚¹ã‚­ãƒ¼ãƒé©ç”¨
            schema_steps = [
                {
                    'description': 'account_history ãƒ†ãƒ¼ãƒ–ãƒ«æ‹¡å¼µ',
                    'sql': """
                        ALTER TABLE account_history ADD COLUMN server_time DATETIME;
                        ALTER TABLE account_history ADD COLUMN trade_allowed BOOLEAN DEFAULT 1;
                        ALTER TABLE account_history ADD COLUMN trade_expert BOOLEAN DEFAULT 1;
                        ALTER TABLE account_history ADD COLUMN margin_so_mode INTEGER DEFAULT 0;
                        ALTER TABLE account_history ADD COLUMN margin_so_call REAL DEFAULT 0;
                        ALTER TABLE account_history ADD COLUMN margin_so_so REAL DEFAULT 0;
                        ALTER TABLE account_history ADD COLUMN currency_digits INTEGER DEFAULT 2;
                        ALTER TABLE account_history ADD COLUMN fifo_close BOOLEAN DEFAULT 0;
                    """
                },
                {
                    'description': 'position_details ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ',
                    'sql': """
                        CREATE TABLE IF NOT EXISTS position_details (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            ticket INTEGER NOT NULL,
                            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                            symbol VARCHAR(10) NOT NULL,
                            type INTEGER NOT NULL,
                            volume REAL NOT NULL,
                            price_open REAL NOT NULL,
                            price_current REAL,
                            sl REAL DEFAULT 0,
                            tp REAL DEFAULT 0,
                            profit REAL NOT NULL,
                            swap REAL DEFAULT 0,
                            commission REAL DEFAULT 0,
                            magic INTEGER DEFAULT 0,
                            comment TEXT,
                            identifier INTEGER,
                            reason INTEGER DEFAULT 0,
                            external_id VARCHAR(50)
                        );
                    """
                },
                {
                    'description': 'statistics_cache ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ',
                    'sql': """
                        CREATE TABLE IF NOT EXISTS statistics_cache (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            calculation_date DATE NOT NULL,
                            period_type VARCHAR(20) NOT NULL,
                            total_trades INTEGER DEFAULT 0,
                            winning_trades INTEGER DEFAULT 0,
                            losing_trades INTEGER DEFAULT 0,
                            gross_profit REAL DEFAULT 0,
                            gross_loss REAL DEFAULT 0,
                            profit_factor REAL DEFAULT 0,
                            expected_payoff REAL DEFAULT 0,
                            absolute_drawdown REAL DEFAULT 0,
                            maximal_drawdown REAL DEFAULT 0,
                            relative_drawdown REAL DEFAULT 0,
                            sharpe_ratio REAL DEFAULT 0,
                            sortino_ratio REAL DEFAULT 0,
                            calmar_ratio REAL DEFAULT 0,
                            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(calculation_date, period_type)
                        );
                    """
                },
                {
                    'description': 'system_monitoring ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ',
                    'sql': """
                        CREATE TABLE IF NOT EXISTS system_monitoring (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                            component VARCHAR(50) NOT NULL,
                            metric_name VARCHAR(50) NOT NULL,
                            metric_value REAL NOT NULL,
                            status VARCHAR(20) DEFAULT 'normal',
                            message TEXT
                        );
                    """
                },
                {
                    'description': 'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ',
                    'sql': """
                        CREATE INDEX IF NOT EXISTS idx_position_details_timestamp ON position_details(timestamp);
                        CREATE INDEX IF NOT EXISTS idx_position_details_ticket ON position_details(ticket);
                        CREATE INDEX IF NOT EXISTS idx_statistics_cache_date_type ON statistics_cache(calculation_date, period_type);
                        CREATE INDEX IF NOT EXISTS idx_system_monitoring_timestamp ON system_monitoring(timestamp);
                        CREATE INDEX IF NOT EXISTS idx_system_monitoring_component ON system_monitoring(component);
                    """
                }
            ]
            
            # å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œ
            for step in schema_steps:
                try:
                    # è¤‡æ•°ã®SQLæ–‡ã‚’åˆ†å‰²å®Ÿè¡Œ
                    sql_statements = [stmt.strip() for stmt in step['sql'].split(';') if stmt.strip()]
                    
                    for sql in sql_statements:
                        cursor.execute(sql)
                    
                    conn.commit()
                    
                    migration_result['steps'].append({
                        'description': step['description'],
                        'status': 'success',
                        'executed_at': datetime.now().isoformat()
                    })
                    
                    logger.info(f"âœ… {step['description']} å®Œäº†")
                    
                except Exception as e:
                    error_msg = f"{step['description']} ã‚¨ãƒ©ãƒ¼: {str(e)}"
                    logger.error(f"âŒ {error_msg}")
                    
                    migration_result['steps'].append({
                        'description': step['description'],
                        'status': 'error',
                        'error': str(e),
                        'executed_at': datetime.now().isoformat()
                    })
                    
                    migration_result['errors'].append(error_msg)
            
            conn.close()
            
            # 3. ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ç¢ºèª
            if not migration_result['errors']:
                migration_result['success'] = True
                logger.info("âœ… Phase 3 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")
            else:
                logger.warning(f"âš ï¸ ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼ˆ{len(migration_result['errors'])}ä»¶ã®ã‚¨ãƒ©ãƒ¼ï¼‰")
            
            migration_result['completed_at'] = datetime.now().isoformat()
            
            return migration_result
            
        except Exception as e:
            logger.error(f"ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'started_at': datetime.now().isoformat()
            }
    
    def optimize_database(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            optimization_result = {
                'started_at': datetime.now().isoformat(),
                'operations': [],
                'success': False
            }
            
            # 1. VACUUMå®Ÿè¡Œ
            cursor.execute("VACUUM")
            optimization_result['operations'].append({
                'operation': 'VACUUM',
                'status': 'completed',
                'description': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€é©åŒ–ãƒ»æ–­ç‰‡åŒ–è§£æ¶ˆ'
            })
            
            # 2. ANALYZEå®Ÿè¡Œ
            cursor.execute("ANALYZE")
            optimization_result['operations'].append({
                'operation': 'ANALYZE',
                'status': 'completed',
                'description': 'ã‚¯ã‚¨ãƒªã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çµ±è¨ˆæƒ…å ±ã®æ›´æ–°'
            })
            
            # 3. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ©ç”¨çŠ¶æ³ç¢ºèª
            cursor.execute("SELECT name FROM sqlite_master WHERE type='index'")
            indexes = cursor.fetchall()
            
            optimization_result['operations'].append({
                'operation': 'INDEX_CHECK',
                'status': 'completed',
                'description': f'{len(indexes)}å€‹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç¢ºèª',
                'details': [idx[0] for idx in indexes]
            })
            
            conn.close()
            
            optimization_result['success'] = True
            optimization_result['completed_at'] = datetime.now().isoformat()
            
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†")
            
            return optimization_result
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'started_at': datetime.now().isoformat()
            }
    
    def run_comprehensive_integrity_check(self) -> Dict:
        """åŒ…æ‹¬çš„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®å¾©å®Ÿè¡Œ"""
        try:
            logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŒ…æ‹¬æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯é–‹å§‹...")
            
            comprehensive_result = {
                'started_at': datetime.now().isoformat(),
                'structure_verification': {},
                'data_integrity': {},
                'migration_result': {},
                'optimization_result': {},
                'overall_status': 'unknown',
                'recommendations': []
            }
            
            # 1. æ§‹é€ æ¤œè¨¼
            structure_info = self.verify_database_structure()
            comprehensive_result['structure_verification'] = structure_info
            
            # 2. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            integrity_info = self.check_data_integrity()
            comprehensive_result['data_integrity'] = integrity_info
            
            # 3. å¿…è¦ã«å¿œã˜ã¦ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            if structure_info.get('status') == 'needs_migration':
                logger.info("ğŸ”§ ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™ã€‚å®Ÿè¡Œä¸­...")
                migration_result = self.perform_migration()
                comprehensive_result['migration_result'] = migration_result
                
                if not migration_result.get('success'):
                    comprehensive_result['recommendations'].append(
                        "ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©å…ƒã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
                    )
            else:
                comprehensive_result['recommendations'].append("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ ã¯æœ€æ–°ã§ã™ã€‚")
            
            # 4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–
            optimization_result = self.optimize_database()
            comprehensive_result['optimization_result'] = optimization_result
            
            # 5. å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
            issues_count = (
                len(structure_info.get('issues', [])) +
                len(integrity_info.get('issues', []))
            )
            
            if issues_count == 0:
                comprehensive_result['overall_status'] = 'excellent'
            elif issues_count <= 2:
                comprehensive_result['overall_status'] = 'good'
            elif issues_count <= 5:
                comprehensive_result['overall_status'] = 'needs_attention'
            else:
                comprehensive_result['overall_status'] = 'critical'
            
            comprehensive_result['completed_at'] = datetime.now().isoformat()
            
            logger.info(f"âœ… åŒ…æ‹¬æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {comprehensive_result['overall_status']}")
            
            return comprehensive_result
            
        except Exception as e:
            logger.error(f"åŒ…æ‹¬æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'started_at': datetime.now().isoformat()
            }
    
    def export_integrity_report(self, results: Dict, filename: str = "database_integrity_report.json"):
        """æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦å‡ºåŠ›"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… æ•´åˆæ€§ãƒ¬ãƒãƒ¼ãƒˆã‚’ {filename} ã«å‡ºåŠ›ã—ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ...")
    
    manager = DatabaseIntegrityManager()
    results = manager.run_comprehensive_integrity_check()
    
    if results and not results.get('error'):
        print(f"âœ… æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {results['overall_status']}")
        print(f"  æ§‹é€ å•é¡Œ: {len(results['structure_verification'].get('issues', []))}ä»¶")
        print(f"  ãƒ‡ãƒ¼ã‚¿å•é¡Œ: {len(results['data_integrity'].get('issues', []))}ä»¶")
        
        if results.get('migration_result'):
            print(f"  ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {'æˆåŠŸ' if results['migration_result']['success'] else 'å¤±æ•—'}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        manager.export_integrity_report(results)
    else:
        print(f"âŒ æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å¤±æ•—: {results.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")