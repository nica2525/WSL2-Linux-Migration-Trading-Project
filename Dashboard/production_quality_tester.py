"""
Production Quality Tester - æœ¬ç•ªé‹ç”¨å“è³ªã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼
è¨­è¨ˆé€šã‚Šã®å“è³ªã§å®Ÿéš›ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
"""

import os
import sys
import time
import json
import logging
import subprocess
import requests
from datetime import datetime
from typing import Dict, List, Optional
import sqlite3
import threading

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProductionQualityTester:
    """æœ¬ç•ªé‹ç”¨å“è³ªãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.test_results = {
            'started_at': datetime.now().isoformat(),
            'quality_tests': {},
            'performance_tests': {},
            'reliability_tests': {},
            'overall_quality': 'unknown',
            'production_ready': False
        }
        
    def test_system_dependencies(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ã®æ¤œè¨¼"""
        try:
            logger.info("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚æ¤œè¨¼é–‹å§‹...")
            
            test_result = {
                'name': 'System Dependencies',
                'started_at': datetime.now().isoformat(),
                'dependencies': {},
                'status': 'unknown'
            }
            
            # å¿…é ˆPython ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
            required_packages = [
                ('numpy', '1.21.0'),
                ('flask', '2.0.0'),
                ('flask_socketio', '5.0.0'),
                ('sqlite3', 'builtin')
            ]
            
            dependencies_ok = []
            
            for package, min_version in required_packages:
                try:
                    if package == 'sqlite3':
                        import sqlite3
                        version = sqlite3.sqlite_version
                        available = True
                    else:
                        module = __import__(package)
                        version = getattr(module, '__version__', 'unknown')
                        available = True
                    
                    test_result['dependencies'][package] = {
                        'available': available,
                        'version': version,
                        'required': min_version
                    }
                    dependencies_ok.append(available)
                    
                except ImportError:
                    test_result['dependencies'][package] = {
                        'available': False,
                        'version': 'not_installed',
                        'required': min_version
                    }
                    dependencies_ok.append(False)
            
            # MT5é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            mt5_files = [
                '/home/trader/.wine/drive_c/Program Files/MetaTrader 5/terminal64.exe',
                '/home/trader/.wine/drive_c/Program Files/MetaTrader 5/logs/'
            ]
            
            for file_path in mt5_files:
                exists = os.path.exists(file_path)
                test_result['dependencies'][f"MT5_{os.path.basename(file_path)}"] = {
                    'available': exists,
                    'path': file_path
                }
                dependencies_ok.append(exists)
            
            # ç·åˆåˆ¤å®š
            success_rate = sum(dependencies_ok) / len(dependencies_ok)
            test_result['status'] = 'passed' if success_rate >= 0.8 else 'failed'
            test_result['success_rate'] = f"{success_rate * 100:.1f}%"
            test_result['completed_at'] = datetime.now().isoformat()
            
            return test_result
            
        except Exception as e:
            logger.error(f"ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'name': 'System Dependencies',
                'status': 'error',
                'error': str(e)
            }
    
    def test_database_performance(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            logger.info("âš¡ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹...")
            
            test_result = {
                'name': 'Database Performance',
                'started_at': datetime.now().isoformat(),
                'metrics': {},
                'status': 'unknown'
            }
            
            conn = sqlite3.connect('dashboard.db')
            cursor = conn.cursor()
            
            # 1. å¤§é‡ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ãƒ†ã‚¹ãƒˆ
            start_time = time.time()
            test_data = []
            
            for i in range(1000):
                test_data.append((
                    f'test_ticket_{i}',
                    datetime.now().isoformat(),
                    'EURUSD',
                    0,
                    0.01,
                    1.17000,
                    10.0,
                    -0.5,
                    -1.0,
                    12345,
                    'performance_test'
                ))
            
            cursor.executemany("""
                INSERT INTO position_details 
                (ticket, timestamp, symbol, type, volume, price_open, profit, swap, commission, magic, comment)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, test_data)
            
            insert_time = time.time() - start_time
            test_result['metrics']['bulk_insert_1000_records'] = {
                'time_seconds': round(insert_time, 3),
                'records_per_second': round(1000 / insert_time, 1),
                'acceptable': insert_time < 5.0
            }
            
            # 2. ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            start_time = time.time()
            cursor.execute("""
                SELECT symbol, COUNT(*), AVG(profit), SUM(profit)
                FROM position_details 
                WHERE comment = 'performance_test'
                GROUP BY symbol
            """)
            results = cursor.fetchall()
            query_time = time.time() - start_time
            
            test_result['metrics']['complex_query'] = {
                'time_seconds': round(query_time, 3),
                'results_count': len(results),
                'acceptable': query_time < 1.0
            }
            
            # 3. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŠ¹æœãƒ†ã‚¹ãƒˆ
            start_time = time.time()
            cursor.execute("""
                SELECT COUNT(*) 
                FROM position_details 
                WHERE timestamp > date('now', '-1 day')
            """)
            count = cursor.fetchone()[0]
            index_query_time = time.time() - start_time
            
            test_result['metrics']['indexed_query'] = {
                'time_seconds': round(index_query_time, 3),
                'records_found': count,
                'acceptable': index_query_time < 0.5
            }
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cursor.execute("DELETE FROM position_details WHERE comment = 'performance_test'")
            conn.commit()
            conn.close()
            
            # ç·åˆåˆ¤å®š
            acceptable_tests = sum(1 for metric in test_result['metrics'].values() if metric['acceptable'])
            total_tests = len(test_result['metrics'])
            
            test_result['status'] = 'passed' if acceptable_tests == total_tests else 'failed'
            test_result['performance_score'] = f"{(acceptable_tests / total_tests * 100):.1f}%"
            test_result['completed_at'] = datetime.now().isoformat()
            
            return test_result
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'name': 'Database Performance',
                'status': 'error',
                'error': str(e)
            }
    
    def test_real_data_integration(self) -> Dict:
        """å®Ÿãƒ‡ãƒ¼ã‚¿çµ±åˆã®å“è³ªãƒ†ã‚¹ãƒˆ"""
        try:
            logger.info("ğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿çµ±åˆå“è³ªãƒ†ã‚¹ãƒˆé–‹å§‹...")
            
            test_result = {
                'name': 'Real Data Integration',
                'started_at': datetime.now().isoformat(),
                'integration_tests': {},
                'status': 'unknown'
            }
            
            # 1. MT5ãƒ‡ãƒ¼ã‚¿è§£æå“è³ª
            from mt5_real_data_parser import MT5RealDataParser
            parser = MT5RealDataParser()
            real_data = parser.get_comprehensive_data()
            
            data_quality = real_data.get('data_quality', 'unknown')
            has_account_info = bool(real_data.get('account_info'))
            has_ea_activity = bool(real_data.get('ea_activity', {}).get('ea_name'))
            
            test_result['integration_tests']['mt5_data_parsing'] = {
                'description': 'MT5ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿è§£æå“è³ª',
                'quality': data_quality,
                'has_account_info': has_account_info,
                'has_ea_activity': has_ea_activity,
                'acceptable': data_quality in ['high', 'medium'] and has_account_info
            }
            
            # 2. çµ±è¨ˆè¨ˆç®—ç²¾åº¦
            from statistics_validator import StatisticsValidator
            validator = StatisticsValidator()
            
            # å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®çµ±è¨ˆè¨ˆç®—ãƒ†ã‚¹ãƒˆ
            test_trades = [
                {'profit': 150.0, 'swap': -2.0, 'commission': -5.0},
                {'profit': -100.0, 'swap': -1.5, 'commission': -5.0},
                {'profit': 200.0, 'swap': -3.0, 'commission': -5.0}
            ]
            
            basic_stats = validator.validate_basic_statistics(test_trades)
            advanced_stats = validator.validate_advanced_statistics(test_trades)
            
            stats_accuracy_ok = (
                basic_stats.get('validation_passed', False) and
                advanced_stats.get('validation', {}).get('passed', False)
            )
            
            test_result['integration_tests']['statistics_accuracy'] = {
                'description': 'çµ±è¨ˆè¨ˆç®—ç²¾åº¦æ¤œè¨¼',
                'basic_stats_passed': basic_stats.get('validation_passed', False),
                'advanced_stats_passed': advanced_stats.get('validation', {}).get('passed', False),
                'acceptable': stats_accuracy_ok
            }
            
            # 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ
            conn = sqlite3.connect('dashboard.db')
            cursor = conn.cursor()
            
            # å¿…é ˆãƒ†ãƒ¼ãƒ–ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ç¢ºèª
            cursor.execute("SELECT COUNT(*) FROM position_details")
            position_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM statistics_cache")
            stats_count = cursor.fetchone()[0]
            
            conn.close()
            
            db_integration_ok = position_count >= 0  # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ã€ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
            
            test_result['integration_tests']['database_integration'] = {
                'description': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆçŠ¶æ³',
                'position_records': position_count,
                'statistics_records': stats_count,
                'tables_accessible': True,
                'acceptable': db_integration_ok
            }
            
            # ç·åˆåˆ¤å®š
            acceptable_tests = sum(1 for test in test_result['integration_tests'].values() if test['acceptable'])
            total_tests = len(test_result['integration_tests'])
            
            test_result['status'] = 'passed' if acceptable_tests >= total_tests * 0.75 else 'failed'
            test_result['integration_score'] = f"{(acceptable_tests / total_tests * 100):.1f}%"
            test_result['completed_at'] = datetime.now().isoformat()
            
            return test_result
            
        except Exception as e:
            logger.error(f"å®Ÿãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'name': 'Real Data Integration',
                'status': 'error',
                'error': str(e)
            }
    
    def test_system_reliability(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ ä¿¡é ¼æ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            logger.info("ğŸ›¡ï¸ ã‚·ã‚¹ãƒ†ãƒ ä¿¡é ¼æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹...")
            
            test_result = {
                'name': 'System Reliability',
                'started_at': datetime.now().isoformat(),
                'reliability_tests': {},
                'status': 'unknown'
            }
            
            # 1. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            error_handling_ok = True
            try:
                # ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ†ã‚¹ãƒˆ
                from real_data_integration import RealDataIntegrationManager
                manager = RealDataIntegrationManager()
                
                # ç©ºã®ãƒ‡ãƒ¼ã‚¿ã§ã®çµ±è¨ˆè¨ˆç®—
                empty_result = manager._calculate_sharpe_ratio([])
                error_handling_ok = error_handling_ok and (empty_result == 0)
                
                # ä¸æ­£ãªå€¤ã§ã®è¨ˆç®—
                invalid_result = manager._calculate_max_drawdown([])
                error_handling_ok = error_handling_ok and (invalid_result == 0)
                
            except Exception as e:
                error_handling_ok = False
            
            test_result['reliability_tests']['error_handling'] = {
                'description': 'ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å …ç‰¢æ€§',
                'handles_empty_data': True,
                'handles_invalid_input': True,
                'acceptable': error_handling_ok
            }
            
            # 2. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            memory_usage = process.memory_info().rss / 1024 / 1024  # MB
            
            memory_efficient = memory_usage < 500  # 500MBæœªæº€
            
            test_result['reliability_tests']['memory_efficiency'] = {
                'description': 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨åŠ¹ç‡',
                'memory_usage_mb': round(memory_usage, 1),
                'acceptable_limit_mb': 500,
                'acceptable': memory_efficient
            }
            
            # 3. ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ“ä½œ
            filesystem_ok = True
            try:
                # ãƒ†ãƒ³ãƒãƒ©ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆãƒ»å‰Šé™¤
                test_file = 'reliability_test.tmp'
                with open(test_file, 'w') as f:
                    f.write('test')
                
                filesystem_ok = os.path.exists(test_file)
                os.remove(test_file)
                filesystem_ok = filesystem_ok and not os.path.exists(test_file)
                
            except Exception as e:
                filesystem_ok = False
            
            test_result['reliability_tests']['filesystem_operations'] = {
                'description': 'ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ“ä½œ',
                'can_create_files': True,
                'can_delete_files': True,
                'acceptable': filesystem_ok
            }
            
            # ç·åˆåˆ¤å®š
            acceptable_tests = sum(1 for test in test_result['reliability_tests'].values() if test['acceptable'])
            total_tests = len(test_result['reliability_tests'])
            
            test_result['status'] = 'passed' if acceptable_tests == total_tests else 'failed'
            test_result['reliability_score'] = f"{(acceptable_tests / total_tests * 100):.1f}%"
            test_result['completed_at'] = datetime.now().isoformat()
            
            return test_result
            
        except Exception as e:
            logger.error(f"ã‚·ã‚¹ãƒ†ãƒ ä¿¡é ¼æ€§ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'name': 'System Reliability',
                'status': 'error',
                'error': str(e)
            }
    
    def run_production_quality_assessment(self) -> Dict:
        """æœ¬ç•ªé‹ç”¨å“è³ªã®åŒ…æ‹¬è©•ä¾¡"""
        try:
            logger.info("ğŸ¯ æœ¬ç•ªé‹ç”¨å“è³ªåŒ…æ‹¬è©•ä¾¡é–‹å§‹...")
            
            # å„å“è³ªãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
            quality_tests = [
                ('quality_tests', self.test_system_dependencies),
                ('performance_tests', self.test_database_performance),
                ('quality_tests', self.test_real_data_integration),
                ('reliability_tests', self.test_system_reliability)
            ]
            
            passed_tests = 0
            total_tests = len(quality_tests)
            
            for category, test_func in quality_tests:
                test_result = test_func()
                test_name = test_result['name']
                
                if category not in self.test_results:
                    self.test_results[category] = {}
                
                self.test_results[category][test_name] = test_result
                
                if test_result['status'] == 'passed':
                    passed_tests += 1
                    logger.info(f"âœ… {test_name}: åˆæ ¼")
                else:
                    logger.warning(f"âŒ {test_name}: ä¸åˆæ ¼")
            
            # å…¨ä½“å“è³ªè©•ä¾¡
            success_rate = passed_tests / total_tests
            
            if success_rate == 1.0:
                self.test_results['overall_quality'] = 'production_ready'
                self.test_results['production_ready'] = True
            elif success_rate >= 0.8:
                self.test_results['overall_quality'] = 'high_quality'
                self.test_results['production_ready'] = True
            elif success_rate >= 0.6:
                self.test_results['overall_quality'] = 'acceptable'
                self.test_results['production_ready'] = False
            else:
                self.test_results['overall_quality'] = 'needs_improvement'
                self.test_results['production_ready'] = False
            
            self.test_results['summary'] = {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'success_rate': f"{success_rate * 100:.1f}%",
                'quality_level': self.test_results['overall_quality'],
                'production_ready': self.test_results['production_ready']
            }
            
            self.test_results['completed_at'] = datetime.now().isoformat()
            
            logger.info(f"ğŸ† æœ¬ç•ªå“è³ªè©•ä¾¡å®Œäº†: {self.test_results['overall_quality']} ({self.test_results['summary']['success_rate']})")
            
            return self.test_results
            
        except Exception as e:
            logger.error(f"æœ¬ç•ªå“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'error': str(e),
                'started_at': datetime.now().isoformat(),
                'overall_quality': 'error'
            }
    
    def export_quality_report(self, filename: str = "production_quality_report.json"):
        """å“è³ªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… æœ¬ç•ªå“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ {filename} ã«å‡ºåŠ›ã—ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"å“è³ªãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    print("ğŸ­ æœ¬ç•ªé‹ç”¨å“è³ªè©•ä¾¡å®Ÿè¡Œ...")
    
    tester = ProductionQualityTester()
    results = tester.run_production_quality_assessment()
    
    if results and not results.get('error'):
        print(f"âœ… æœ¬ç•ªå“è³ªè©•ä¾¡å®Œäº†: {results['overall_quality']}")
        print(f"  å“è³ªæˆåŠŸç‡: {results['summary']['success_rate']}")
        print(f"  æœ¬ç•ªé‹ç”¨å¯èƒ½: {'ã¯ã„' if results['production_ready'] else 'ã„ã„ãˆ'}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        tester.export_quality_report()
    else:
        print(f"âŒ æœ¬ç•ªå“è³ªè©•ä¾¡å¤±æ•—: {results.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")