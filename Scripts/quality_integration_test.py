#!/usr/bin/env python3
"""
å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ
å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã™ã‚‹å‹•ä½œæ¤œè¨¼
"""

import json
import os
import sys
from pathlib import Path

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from quality_checker import QualityChecker


def test_actual_project_files():
    """å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ” å®Ÿãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å“è³ªæ¤œè¨¼é–‹å§‹")

    project_root = Path(__file__).parent.parent
    checker = QualityChecker(project_root)

    # å®Ÿéš›ã®ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œ
    issues = checker.scan_quality_issues()

    print("\nğŸ“Š æ¤œå‡ºçµæœ:")
    print(f"   ç·å•é¡Œæ•°: {len(issues)}")

    # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥é›†è¨ˆ
    file_counts = {}
    pattern_counts = {}
    confidence_sum = 0

    for issue in issues:
        file_name = Path(issue["file"]).name
        file_counts[file_name] = file_counts.get(file_name, 0) + 1

        pattern = issue["type"]
        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1

        confidence_sum += issue["confidence"]

    print("\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥å•é¡Œæ•°:")
    for file_name, count in sorted(file_counts.items()):
        print(f"   {file_name}: {count}ä»¶")

    print("\nğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥é »åº¦:")
    for pattern, count in sorted(pattern_counts.items()):
        print(f"   {pattern}: {count}ä»¶")

    avg_confidence = confidence_sum / len(issues) if issues else 0
    print("\nğŸ“ˆ æ¤œå‡ºå“è³ª:")
    print(f"   å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.1%}")

    # é«˜ä¿¡é ¼åº¦å•é¡Œã®ç¢ºèª
    high_confidence_issues = [i for i in issues if i["confidence"] >= 0.9]
    print(f"   é«˜ä¿¡é ¼åº¦å•é¡Œ: {len(high_confidence_issues)}ä»¶")

    # False positiveå¯èƒ½æ€§ã®ãƒã‚§ãƒƒã‚¯
    potential_fps = []
    for issue in issues:
        if any(
            fp_pattern in issue.get("description", "")
            for fp_pattern in ["æ™‚é–“åˆ‡ã‚Œ", "TIME_EXIT", "FORCED_EXIT", "å¼·åˆ¶æ±ºæ¸ˆ"]
        ):
            potential_fps.append(issue)

    print(f"   False positiveå€™è£œ: {len(potential_fps)}ä»¶")

    # å•é¡Œã®ã‚ã‚‹æ¤œå‡ºãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    validation_results = {
        "total_issues": len(issues),
        "avg_confidence": avg_confidence,
        "high_confidence_count": len(high_confidence_issues),
        "potential_false_positives": len(potential_fps),
        "file_coverage": len(file_counts),
        "pattern_diversity": len(pattern_counts),
    }

    return validation_results


def validate_detection_accuracy():
    """æ¤œå‡ºç²¾åº¦ã®å¦¥å½“æ€§æ¤œè¨¼"""
    print("\nğŸ¯ æ¤œå‡ºç²¾åº¦å¦¥å½“æ€§æ¤œè¨¼")

    results = test_actual_project_files()

    # å“è³ªåŸºæº–ãƒã‚§ãƒƒã‚¯
    checks = {
        "å¹³å‡ä¿¡é ¼åº¦ >= 80%": results["avg_confidence"] >= 0.8,
        "é«˜ä¿¡é ¼åº¦å•é¡Œ >= 1ä»¶": results["high_confidence_count"] >= 1,
        "False positiveç‡ < 20%": (
            results["potential_false_positives"] / max(results["total_issues"], 1)
        )
        < 0.2,
        "ãƒ‘ã‚¿ãƒ¼ãƒ³å¤šæ§˜æ€§ >= 2ç¨®é¡": results["pattern_diversity"] >= 2,
        "ãƒ•ã‚¡ã‚¤ãƒ«ã‚«ãƒãƒ¬ãƒƒã‚¸ >= 2ä»¶": results["file_coverage"] >= 2,
    }

    passed_checks = sum(checks.values())
    total_checks = len(checks)

    print(f"\nâœ… å“è³ªåŸºæº–ãƒã‚§ãƒƒã‚¯çµæœ: {passed_checks}/{total_checks}")
    for criterion, passed in checks.items():
        status = "âœ…" if passed else "âŒ"
        print(f"   {status} {criterion}")

    # ç·åˆåˆ¤å®š
    quality_score = passed_checks / total_checks
    if quality_score >= 0.8:
        print(f"\nğŸŠ å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼å‹•ä½œä¿è¨¼: åˆæ ¼ ({quality_score:.1%})")
        return True
    else:
        print(f"\nâš ï¸ å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼å‹•ä½œä¿è¨¼: è¦æ”¹å–„ ({quality_score:.1%})")
        return False


def generate_validation_report():
    """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    print("ğŸ“ å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")

    results = test_actual_project_files()
    validation_passed = validate_detection_accuracy()

    report = {
        "validation_timestamp": "2025-07-13T19:35:00JST",
        "validation_status": "PASSED" if validation_passed else "FAILED",
        "detection_results": results,
        "validation_criteria": {
            "min_avg_confidence": 0.8,
            "min_high_confidence_issues": 1,
            "max_false_positive_rate": 0.2,
            "min_pattern_diversity": 2,
            "min_file_coverage": 2,
        },
        "recommendations": [
            "GeminiæŸ»èª­ã§ã®ç²¾åº¦å‘ä¸Šæ¤œè¨" if not validation_passed else "ç¾çŠ¶ã®æ¤œå‡ºç²¾åº¦ã§é‹ç”¨å¯èƒ½",
            "å®šæœŸçš„ãªæ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ",
            "æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³è¿½åŠ æ™‚ã®æ¤œè¨¼å¼·åŒ–",
        ],
    }

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_file = (
        Path(__file__).parent.parent / "quality_checker_validation_report.json"
    )
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file.name}")
    return validation_passed


if __name__ == "__main__":
    success = generate_validation_report()
    print(f"\nğŸ”¬ å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼å‹•ä½œä¿è¨¼: {'âœ… å®Œäº†' if success else 'âŒ è¦æ”¹å–„'}")
    exit(0 if success else 1)
