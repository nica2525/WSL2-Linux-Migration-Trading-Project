#!/usr/bin/env python3
"""
MT5 Excelå®Œå…¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ è§£æã‚·ã‚¹ãƒ†ãƒ 
ç›®çš„: ã„ã„åŠ æ¸›ãªåˆ†æã®æ’é™¤ãƒ»å®Œç’§ãªãƒ‡ãƒ¼ã‚¿ç†è§£
å“è³ªåŸºæº–: æœ€é«˜æ°´æº–ãƒ»å…¨è¡Œå…¨åˆ—è©³ç´°åˆ†æãƒ»ä»®å®šæ’é™¤

ä½œæˆè€…: Claude (ãƒ‡ãƒ¼ã‚¿æ§‹é€ ç†è§£å°‚é–€æ‹…å½“)
"""

import json
import os
import re
from datetime import datetime
from typing import Dict, List

import pandas as pd


class MT5DataStructureAnalyzer:
    """MT5 Excelå®Œå…¨æ§‹é€ è§£æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, excel_path: str):
        self.excel_path = excel_path
        self.raw_data = None
        self.structure_analysis = {}

    def load_complete_excel_structure(self) -> bool:
        """Excelå®Œå…¨æ§‹é€ èª­ã¿è¾¼ã¿ãƒ»å…¨æƒ…å ±ä¿æŒ"""
        print("ğŸ” === MT5 Excelå®Œå…¨æ§‹é€ è§£æé–‹å§‹ ===")
        print("å“è³ªåŸºæº–: æœ€é«˜æ°´æº–ãƒ»ä»®å®šæ’é™¤ãƒ»å®Œç’§ç†è§£")
        print()

        try:
            # Excelå…¨æƒ…å ±èª­ã¿è¾¼ã¿
            with pd.ExcelFile(self.excel_path) as xls:
                print(f"ğŸ“„ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(self.excel_path)}")
                print(
                    f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(self.excel_path) / (1024*1024):.1f}MB"
                )

                # å…¨ã‚·ãƒ¼ãƒˆæƒ…å ±
                sheet_names = xls.sheet_names
                print(f"ğŸ“‹ ã‚·ãƒ¼ãƒˆæ•°: {len(sheet_names)}")
                print(f"ğŸ“‹ ã‚·ãƒ¼ãƒˆå: {sheet_names}")

                # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå…¨è¡Œãƒ»å®Œå…¨æƒ…å ±ï¼‰
                print("\nğŸ“Š === å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå®Œå…¨ç‰ˆï¼‰ ===")
                self.raw_data = pd.read_excel(xls, sheet_name="Sheet1", header=None)

                # åŸºæœ¬æƒ…å ±
                print(f"âœ… ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {self.raw_data.shape}")
                print(
                    f"âœ… ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {self.raw_data.memory_usage(deep=True).sum() / (1024*1024):.1f}MB"
                )

                return True

        except Exception as e:
            print(f"âŒ Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            traceback.print_exc()
            return False

    def analyze_complete_structure(self) -> Dict:
        """å®Œå…¨æ§‹é€ åˆ†æãƒ»å…¨è¡Œå…¨åˆ—è©³ç´°è§£æ"""
        print("\nğŸ” === å®Œå…¨æ§‹é€ åˆ†æå®Ÿè¡Œ ===")

        if self.raw_data is None:
            return {}

        analysis = {
            "basic_info": self._analyze_basic_info(),
            "cell_content_analysis": self._analyze_cell_contents(),
            "section_identification": self._identify_all_sections(),
            "data_type_analysis": self._analyze_data_types(),
            "pattern_analysis": self._analyze_patterns(),
            "quality_assessment": self._assess_data_quality(),
        }

        self.structure_analysis = analysis
        return analysis

    def _analyze_basic_info(self) -> Dict:
        """åŸºæœ¬æƒ…å ±è©³ç´°åˆ†æ"""
        print("ğŸ“Š åŸºæœ¬æƒ…å ±åˆ†æ...")

        rows, cols = self.raw_data.shape

        # å„åˆ—ã®éNULLå€¤æ•°
        col_info = []
        for col_idx in range(cols):
            col_data = self.raw_data.iloc[:, col_idx]
            non_null_count = col_data.notna().sum()
            null_count = col_data.isna().sum()

            # ãƒ‡ãƒ¼ã‚¿å‹åˆ†æ
            unique_types = set()
            for val in col_data.dropna().head(100):  # æœ€åˆã®100å€‹ã‚’ã‚µãƒ³ãƒ—ãƒ«
                unique_types.add(type(val).__name__)

            col_info.append(
                {
                    "column_index": col_idx,
                    "non_null_count": non_null_count,
                    "null_count": null_count,
                    "data_types": list(unique_types),
                    "sample_values": col_data.dropna().head(10).tolist(),
                }
            )

        return {"total_rows": rows, "total_columns": cols, "column_analysis": col_info}

    def _analyze_cell_contents(self) -> Dict:
        """ã‚»ãƒ«å†…å®¹å®Œå…¨åˆ†æãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®š"""
        print("ğŸ“Š ã‚»ãƒ«å†…å®¹å®Œå…¨åˆ†æ...")

        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®šç¾©ï¼ˆMT5ãƒ»å–å¼•é–¢é€£ï¼‰
        keywords = {
            "time_related": ["Time", "Date", "æ™‚é–“", "æ™‚åˆ»", "æ—¥æ™‚", "datetime"],
            "order_related": ["Order", "Deal", "Position", "æ³¨æ–‡", "ç´„å®š", "ã‚ªãƒ¼ãƒ€ãƒ¼"],
            "trade_related": [
                "Buy",
                "Sell",
                "Type",
                "Volume",
                "Size",
                "è²·ã„",
                "å£²ã‚Š",
                "æ•°é‡",
            ],
            "price_related": [
                "Price",
                "Open",
                "Close",
                "High",
                "Low",
                "ä¾¡æ ¼",
                "å§‹å€¤",
                "çµ‚å€¤",
            ],
            "profit_related": [
                "Profit",
                "Loss",
                "P&L",
                "Swap",
                "Commission",
                "æç›Š",
                "åˆ©ç›Š",
            ],
            "symbol_related": [
                "Symbol",
                "Pair",
                "Currency",
                "é€šè²¨",
                "éŠ˜æŸ„",
                "EURUSD",
                "USDJPY",
            ],
            "status_related": ["Status", "State", "Filled", "Cancelled", "çŠ¶æ…‹", "çŠ¶æ³"],
            "sl_tp_related": ["S/L", "T/P", "Stop", "Limit", "æ±ºæ¸ˆ", "æŒ‡å€¤", "é€†æŒ‡å€¤"],
        }

        keyword_analysis = {}
        cell_patterns = []

        # å…¨è¡Œã‚’ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆåŠ¹ç‡åŒ–ã®ãŸã‚1000è¡Œãšã¤å‡¦ç†ï¼‰
        batch_size = 1000
        total_rows = len(self.raw_data)

        for start_row in range(0, min(total_rows, 5000), batch_size):  # æœ€åˆã®5000è¡Œã‚’è©³ç´°åˆ†æ
            end_row = min(start_row + batch_size, total_rows)
            print(f"  åˆ†æä¸­: è¡Œ{start_row}-{end_row}/{total_rows}")

            batch_data = self.raw_data.iloc[start_row:end_row]

            for row_idx, row in batch_data.iterrows():
                # è¡Œå…¨ä½“ã‚’æ–‡å­—åˆ—ã¨ã—ã¦çµåˆ
                row_text = " ".join([str(val) for val in row.values if pd.notna(val)])

                if not row_text.strip():
                    continue

                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                matched_categories = []
                for category, words in keywords.items():
                    matches = sum(
                        1 for word in words if word.lower() in row_text.lower()
                    )
                    if matches > 0:
                        matched_categories.append(
                            {
                                "category": category,
                                "matches": matches,
                                "matched_words": [
                                    word
                                    for word in words
                                    if word.lower() in row_text.lower()
                                ],
                            }
                        )

                # é‡è¦è¡Œã®ç‰¹å®šï¼ˆå¤šæ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒï¼‰
                total_matches = sum(cat["matches"] for cat in matched_categories)
                if total_matches >= 3:
                    cell_patterns.append(
                        {
                            "row_index": row_idx,
                            "total_keyword_matches": total_matches,
                            "matched_categories": matched_categories,
                            "row_content": row.values.tolist(),
                            "row_text_preview": row_text[:200]
                            + ("..." if len(row_text) > 200 else ""),
                        }
                    )

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµ±è¨ˆ
        for category in keywords.keys():
            category_rows = [
                p
                for p in cell_patterns
                if any(c["category"] == category for c in p["matched_categories"])
            ]
            keyword_analysis[category] = {
                "total_occurrences": len(category_rows),
                "top_rows": sorted(
                    category_rows,
                    key=lambda x: x["total_keyword_matches"],
                    reverse=True,
                )[:5],
            }

        return {
            "keyword_analysis": keyword_analysis,
            "potential_header_rows": sorted(
                cell_patterns, key=lambda x: x["total_keyword_matches"], reverse=True
            )[:20],
            "total_analyzed_rows": min(total_rows, 5000),
        }

    def _identify_all_sections(self) -> Dict:
        """å…¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç‰¹å®šãƒ»éšå±¤æ§‹é€ ç†è§£"""
        print("ğŸ“Š å…¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç‰¹å®šãƒ»éšå±¤æ§‹é€ åˆ†æ...")

        sections = []
        current_section = None

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å¢ƒç•Œãƒ‘ã‚¿ãƒ¼ãƒ³
        section_patterns = [
            r"^[A-Z][a-zA-Z\s]+:",  # "Settings:" å½¢å¼
            r"^\d+\.",  # "1." å½¢å¼
            r"^={3,}",  # "===" å½¢å¼
            r"^-{3,}",  # "---" å½¢å¼
            r"^\w+\s*\w*:",  # "é …ç›®:" å½¢å¼
        ]

        for row_idx in range(min(len(self.raw_data), 1000)):  # æœ€åˆã®1000è¡Œã‚’åˆ†æ
            row = self.raw_data.iloc[row_idx]
            first_cell = str(row.iloc[0]) if pd.notna(row.iloc[0]) else ""

            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å¢ƒç•Œæ¤œå‡º
            is_section_boundary = False
            for pattern in section_patterns:
                if re.match(pattern, first_cell):
                    is_section_boundary = True
                    break

            # æ–°ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
            if is_section_boundary or (
                first_cell
                and len(first_cell) > 3
                and not any(c.isdigit() for c in first_cell[:10])
            ):
                if current_section:
                    current_section["end_row"] = row_idx - 1
                    current_section["row_count"] = (
                        current_section["end_row"] - current_section["start_row"] + 1
                    )
                    sections.append(current_section)

                current_section = {
                    "section_id": len(sections),
                    "start_row": row_idx,
                    "section_header": first_cell,
                    "content_preview": row.values.tolist(),
                }

        # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç†
        if current_section:
            current_section["end_row"] = len(self.raw_data) - 1
            current_section["row_count"] = (
                current_section["end_row"] - current_section["start_row"] + 1
            )
            sections.append(current_section)

        return {"total_sections": len(sections), "sections": sections}

    def _analyze_data_types(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿å‹è©³ç´°åˆ†æ"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‹è©³ç´°åˆ†æ...")

        type_analysis = {}

        for col_idx in range(self.raw_data.shape[1]):
            col_data = self.raw_data.iloc[:, col_idx].dropna()

            if len(col_data) == 0:
                continue

            # å‹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            type_patterns = {"datetime": 0, "numeric": 0, "text": 0, "mixed": 0}

            sample_size = min(len(col_data), 100)
            sample_data = col_data.head(sample_size)

            for val in sample_data:
                val_str = str(val)

                # æ—¥æ™‚ãƒ‘ã‚¿ãƒ¼ãƒ³
                if re.match(r"\d{4}[.\-/]\d{2}[.\-/]\d{2}", val_str):
                    type_patterns["datetime"] += 1
                # æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
                elif re.match(r"^[+-]?\d*\.?\d+$", val_str.replace(",", "")):
                    type_patterns["numeric"] += 1
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³
                else:
                    type_patterns["text"] += 1

            # ä¸»è¦å‹æ±ºå®š
            dominant_type = max(type_patterns, key=type_patterns.get)

            type_analysis[f"column_{col_idx}"] = {
                "dominant_type": dominant_type,
                "type_distribution": type_patterns,
                "sample_values": sample_data.head(5).tolist(),
            }

        return type_analysis

    def _analyze_patterns(self) -> Dict:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãƒ»è¦å‰‡æ€§ç™ºè¦‹"""
        print("ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãƒ»è¦å‰‡æ€§ç™ºè¦‹...")

        patterns = {
            "repeating_structures": [],
            "column_relationships": [],
            "data_sequences": [],
        }

        # åˆ—é–“é–¢ä¿‚åˆ†æ
        for col1_idx in range(min(self.raw_data.shape[1], 5)):  # æœ€åˆã®5åˆ—ã®ã¿
            for col2_idx in range(col1_idx + 1, min(self.raw_data.shape[1], 5)):
                col1_data = self.raw_data.iloc[:, col1_idx].dropna()
                col2_data = self.raw_data.iloc[:, col2_idx].dropna()

                # ç›¸é–¢é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ï¼ˆæ•°å€¤ã®å ´åˆï¼‰
                try:
                    col1_numeric = pd.to_numeric(col1_data, errors="coerce").dropna()
                    col2_numeric = pd.to_numeric(col2_data, errors="coerce").dropna()

                    if len(col1_numeric) > 10 and len(col2_numeric) > 10:
                        # å…±é€šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ç›¸é–¢è¨ˆç®—
                        common_idx = col1_numeric.index.intersection(col2_numeric.index)
                        if len(common_idx) > 10:
                            correlation = col1_numeric.loc[common_idx].corr(
                                col2_numeric.loc[common_idx]
                            )
                            if abs(correlation) > 0.5:
                                patterns["column_relationships"].append(
                                    {
                                        "col1": col1_idx,
                                        "col2": col2_idx,
                                        "correlation": correlation,
                                        "relationship_type": "numeric_correlation",
                                    }
                                )
                except:
                    pass

        return patterns

    def _assess_data_quality(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡...")

        quality_metrics = {"completeness": {}, "consistency": {}, "validity": {}}

        # å®Œå…¨æ€§è©•ä¾¡
        total_cells = self.raw_data.size
        non_null_cells = self.raw_data.notna().sum().sum()
        quality_metrics["completeness"] = {
            "total_cells": total_cells,
            "non_null_cells": non_null_cells,
            "completeness_ratio": non_null_cells / total_cells,
        }

        # ä¸€è²«æ€§è©•ä¾¡ï¼ˆåˆ—ã”ã¨ï¼‰
        for col_idx in range(self.raw_data.shape[1]):
            col_data = self.raw_data.iloc[:, col_idx].dropna()

            if len(col_data) > 0:
                # ãƒ‡ãƒ¼ã‚¿å‹ä¸€è²«æ€§
                type_consistency = (
                    len({type(val).__name__ for val in col_data.head(50)}) <= 2
                )
                quality_metrics["consistency"][f"column_{col_idx}"] = {
                    "type_consistency": type_consistency,
                    "unique_types": list(
                        {type(val).__name__ for val in col_data.head(20)}
                    ),
                }

        return quality_metrics

    def generate_complete_report(self) -> Dict:
        """å®Œå…¨åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\nğŸ“Š === å®Œå…¨åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ===")

        report = {
            "analysis_timestamp": datetime.now().isoformat(),
            "file_info": {
                "path": self.excel_path,
                "size_mb": os.path.getsize(self.excel_path) / (1024 * 1024),
            },
            "structure_analysis": self.structure_analysis,
            "quality_score": self._calculate_quality_score(),
            "recommendations": self._generate_recommendations(),
        }

        return report

    def _calculate_quality_score(self) -> float:
        """å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º"""
        if not self.structure_analysis:
            return 0.0

        scores = []

        # å®Œå…¨æ€§ã‚¹ã‚³ã‚¢
        if "quality_assessment" in self.structure_analysis:
            completeness = self.structure_analysis["quality_assessment"][
                "completeness"
            ]["completeness_ratio"]
            scores.append(completeness * 100)

        # æ§‹é€ ç†è§£ã‚¹ã‚³ã‚¢
        if "cell_content_analysis" in self.structure_analysis:
            header_rows = len(
                self.structure_analysis["cell_content_analysis"][
                    "potential_header_rows"
                ]
            )
            structure_score = min(header_rows * 5, 100)  # æœ€å¤§100ç‚¹
            scores.append(structure_score)

        return sum(scores) / len(scores) if scores else 0.0

    def _generate_recommendations(self) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        if not self.structure_analysis:
            return ["æ§‹é€ åˆ†æã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„"]

        # å“è³ªãƒ™ãƒ¼ã‚¹æ¨å¥¨
        quality_score = self._calculate_quality_score()

        if quality_score < 50:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿å“è³ªãŒä½ã„ãŸã‚ã€æ‰‹å‹•æ¤œè¨¼ã‚’å¼·ãæ¨å¥¨")

        if quality_score > 80:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿å“è³ªè‰¯å¥½ã€è‡ªå‹•åˆ†æç¶™ç¶šå¯èƒ½")

        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œæ¨å¥¨
        if "cell_content_analysis" in self.structure_analysis:
            header_candidates = self.structure_analysis["cell_content_analysis"][
                "potential_header_rows"
            ]
            if header_candidates:
                best_header = header_candidates[0]
                recommendations.append(
                    f"æœ€æœ‰åŠ›ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ: {best_header['row_index']} (ãƒãƒƒãƒæ•°: {best_header['total_keyword_matches']})"
                )

        return recommendations


def main():
    """å®Œå…¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ è§£æå®Ÿè¡Œ"""
    print("ğŸ” === MT5 Excelå®Œå…¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ è§£æã‚·ã‚¹ãƒ†ãƒ  ===")
    print("å“è³ªåŸºæº–: æœ€é«˜æ°´æº–ãƒ»ä»®å®šæ’é™¤ãƒ»å®Œç’§ç†è§£")
    print("=" * 60)

    excel_path = "/home/trader/Trading-Development/2.ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ‰‹æ³•ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/MT5_Results/Reportãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ-900179988.xlsx"

    analyzer = MT5DataStructureAnalyzer(excel_path)

    # å®Œå…¨æ§‹é€ èª­ã¿è¾¼ã¿
    if not analyzer.load_complete_excel_structure():
        return

    # å®Œå…¨æ§‹é€ åˆ†æ
    analyzer.analyze_complete_structure()

    # å®Œå…¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    complete_report = analyzer.generate_complete_report()

    # çµæœä¿å­˜
    output_path = "/home/trader/Trading-Development/2.ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ‰‹æ³•ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/MT5_Results/complete_structure_analysis.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(complete_report, f, indent=2, ensure_ascii=False, default=str)

    print("\nâœ… å®Œå…¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ è§£æå®Œäº†")
    print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")
    print(f"ğŸ¯ å“è³ªã‚¹ã‚³ã‚¢: {complete_report['quality_score']:.1f}/100")

    # é‡è¦ç™ºè¦‹äº‹é …è¡¨ç¤º
    if complete_report["recommendations"]:
        print("\nğŸ“‹ é‡è¦æ¨å¥¨äº‹é …:")
        for rec in complete_report["recommendations"]:
            print(f"  - {rec}")

    return complete_report


if __name__ == "__main__":
    main()
