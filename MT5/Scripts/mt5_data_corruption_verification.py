#!/usr/bin/env python3
"""
MT5ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬ç²¾å¯†æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
ç›®çš„: MT5ãƒ‡ãƒ¼ã‚¿ãŒä½•ã‚‰ã‹ã®ç ´æãƒ»ä¸æ•´åˆã‚’èµ·ã“ã—ã¦ã„ã‚‹å¯èƒ½æ€§ã®å¾¹åº•æ¤œè¨¼

æ¤œè¨¼é …ç›®:
1. é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®è©³ç´°åˆ†æ
2. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †åºæ•´åˆæ€§æ¤œè¨¼
3. ãƒ‡ãƒ¼ã‚¿æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
4. ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ æ•´åˆæ€§æ¤œè¨¼
5. è«–ç†çš„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯

ä½œæˆè€…: Claudeï¼ˆãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬å°‚é–€æ¤œè¨¼æ‹…å½“ï¼‰
"""

import json
import logging
from datetime import datetime
from typing import Dict

import numpy as np
import pandas as pd


class MT5DataCorruptionVerification:
    """MT5ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬ç²¾å¯†æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, excel_path: str):
        self.excel_path = excel_path
        self.raw_data = None
        self.trades_df = None

        logging.basicConfig(
            level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
        )
        self.logger = logging.getLogger(__name__)

    def load_data_for_corruption_analysis(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ç ´æåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            self.raw_data = pd.read_excel(self.excel_path, header=None)

            header_row = 59
            data_start_row = 60

            header = self.raw_data.iloc[header_row].values
            data_rows = self.raw_data.iloc[data_start_row:].values

            self.trades_df = pd.DataFrame(data_rows, columns=header)
            self.trades_df = self.trades_df.dropna(how="all")

            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ç ´æåˆ†æãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.trades_df)}ä»¶")
            return True

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def analyze_duplicate_timestamps(self) -> Dict:
        """é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è©³ç´°åˆ†æ"""
        self.logger.info("=== é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è©³ç´°åˆ†æ ===")

        analysis = {
            "duplicate_statistics": {},
            "duplicate_patterns": [],
            "temporal_clustering": {},
            "data_integrity_impact": {},
        }

        # åˆ—å®šç¾©
        time_col = self.trades_df.columns[0]
        order_col = self.trades_df.columns[1]
        comment_col = self.trades_df.columns[12]

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é‡è¤‡çµ±è¨ˆ
        timestamp_counts = self.trades_df[time_col].value_counts()
        duplicates = timestamp_counts[timestamp_counts > 1]

        analysis["duplicate_statistics"] = {
            "total_timestamps": len(timestamp_counts),
            "unique_timestamps": len(timestamp_counts),
            "duplicate_timestamps": len(duplicates),
            "max_duplicates_per_timestamp": int(duplicates.max())
            if len(duplicates) > 0
            else 0,
            "total_duplicate_records": int(duplicates.sum() - len(duplicates))
            if len(duplicates) > 0
            else 0,
        }

        # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³è©³ç´°åˆ†æ
        duplicate_patterns = []
        for timestamp, count in duplicates.head(20).items():  # ä¸Šä½20å€‹ã®é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            duplicate_records = self.trades_df[self.trades_df[time_col] == timestamp]

            # åŒä¸€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å†…ã®ãƒ‡ãƒ¼ã‚¿åˆ†æ
            position_ids = duplicate_records[order_col].tolist()
            comments = duplicate_records[comment_col].astype(str).tolist()

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡
            jamesorb_count = sum(1 for c in comments if "JamesORB" in str(c))
            sl_count = sum(1 for c in comments if "sl " in str(c))
            tp_count = sum(1 for c in comments if "tp " in str(c))

            duplicate_patterns.append(
                {
                    "timestamp": str(timestamp),
                    "duplicate_count": int(count),
                    "position_ids": position_ids[:10],  # æœ€åˆã®10å€‹ã®ã¿
                    "comment_distribution": {
                        "JamesORB": jamesorb_count,
                        "stop_loss": sl_count,
                        "take_profit": tp_count,
                        "other": len(comments) - jamesorb_count - sl_count - tp_count,
                    },
                    "data_consistency": {
                        "unique_position_ids": len(set(position_ids)),
                        "position_id_range": [min(position_ids), max(position_ids)]
                        if position_ids
                        else [None, None],
                    },
                }
            )

        analysis["duplicate_patterns"] = duplicate_patterns

        # æ™‚ç³»åˆ—ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ
        if len(duplicates) > 0:
            # é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®æ™‚ç³»åˆ—åˆ†å¸ƒ
            duplicate_timestamps = list(duplicates.index)

            try:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’datetimeã«å¤‰æ›
                duplicate_datetimes = [
                    pd.to_datetime(ts, format="%Y.%m.%d %H:%M:%S")
                    for ts in duplicate_timestamps[:100]  # æœ€åˆã®100å€‹
                ]

                # æ™‚é–“é–“éš”åˆ†æ
                if len(duplicate_datetimes) > 1:
                    duplicate_datetimes.sort()
                    intervals = [
                        (
                            duplicate_datetimes[i + 1] - duplicate_datetimes[i]
                        ).total_seconds()
                        / 60
                        for i in range(len(duplicate_datetimes) - 1)
                    ]

                    analysis["temporal_clustering"] = {
                        "avg_interval_minutes": float(np.mean(intervals)),
                        "min_interval_minutes": float(np.min(intervals)),
                        "max_interval_minutes": float(np.max(intervals)),
                        "intervals_under_1min": sum(1 for i in intervals if i < 1),
                        "intervals_under_1hour": sum(1 for i in intervals if i < 60),
                    }
            except Exception as e:
                self.logger.warning(f"æ™‚ç³»åˆ—ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

        self.logger.info(
            f"é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {analysis['duplicate_statistics']['duplicate_timestamps']}å€‹"
        )
        self.logger.info(
            f"æœ€å¤§é‡è¤‡æ•°: {analysis['duplicate_statistics']['max_duplicates_per_timestamp']}"
        )

        return analysis

    def verify_timestamp_sequence_integrity(self) -> Dict:
        """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †åºæ•´åˆæ€§æ¤œè¨¼"""
        self.logger.info("=== ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †åºæ•´åˆæ€§æ¤œè¨¼ ===")

        integrity_analysis = {
            "sequence_violations": [],
            "chronological_gaps": {},
            "reverse_sequences": {},
            "data_order_consistency": {},
        }

        time_col = self.trades_df.columns[0]
        self.trades_df.columns[1]

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ datetime ã«å¤‰æ›
        valid_timestamps = []
        invalid_count = 0

        for idx, timestamp in enumerate(self.trades_df[time_col]):
            try:
                dt = pd.to_datetime(timestamp, format="%Y.%m.%d %H:%M:%S")
                valid_timestamps.append((idx, dt, timestamp))
            except:
                invalid_count += 1
                if invalid_count <= 10:  # æœ€åˆã®10å€‹ã®ç„¡åŠ¹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨˜éŒ²
                    integrity_analysis["sequence_violations"].append(
                        {
                            "row_index": idx,
                            "invalid_timestamp": str(timestamp),
                            "issue": "invalid_format",
                        }
                    )

        # æ™‚ç³»åˆ—é †åºãƒã‚§ãƒƒã‚¯
        if len(valid_timestamps) > 1:
            reverse_sequences = 0
            large_gaps = 0

            for i in range(len(valid_timestamps) - 1):
                current_idx, current_dt, current_ts = valid_timestamps[i]
                next_idx, next_dt, next_ts = valid_timestamps[i + 1]

                # é€†é †åºãƒã‚§ãƒƒã‚¯
                if next_dt < current_dt:
                    reverse_sequences += 1
                    if len(integrity_analysis["sequence_violations"]) < 50:
                        integrity_analysis["sequence_violations"].append(
                            {
                                "row_indices": [current_idx, next_idx],
                                "timestamps": [str(current_ts), str(next_ts)],
                                "time_difference_hours": (
                                    current_dt - next_dt
                                ).total_seconds()
                                / 3600,
                                "issue": "reverse_sequence",
                            }
                        )

                # ç•°å¸¸ãªæ™‚é–“ã‚®ãƒ£ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯ï¼ˆ1é€±é–“ä»¥ä¸Šï¼‰
                time_gap = abs((next_dt - current_dt).total_seconds() / 3600)
                if time_gap > 168:  # 1é€±é–“
                    large_gaps += 1

            integrity_analysis["reverse_sequences"] = {
                "total_reverse_sequences": reverse_sequences,
                "reverse_sequence_rate": reverse_sequences / len(valid_timestamps),
            }

            integrity_analysis["chronological_gaps"] = {
                "large_gaps_count": large_gaps,
                "valid_timestamp_pairs": len(valid_timestamps) - 1,
            }

        self.logger.info(f"ç„¡åŠ¹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {invalid_count}å€‹")
        self.logger.info(
            f"é€†é †åºã‚·ãƒ¼ã‚±ãƒ³ã‚¹: {integrity_analysis.get('reverse_sequences', {}).get('total_reverse_sequences', 0)}å€‹"
        )

        return integrity_analysis

    def analyze_data_missing_patterns(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ"""
        self.logger.info("=== ãƒ‡ãƒ¼ã‚¿æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ ===")

        missing_analysis = {
            "column_missing_stats": {},
            "missing_patterns": {},
            "data_completeness_score": 0.0,
            "systematic_missing_evidence": {},
        }

        # åˆ—åˆ¥æ¬ æçµ±è¨ˆ
        total_rows = len(self.trades_df)

        for i, col in enumerate(self.trades_df.columns):
            if pd.isna(col):
                col_name = f"column_{i}_unnamed"
            else:
                col_name = str(col)

            missing_count = self.trades_df.iloc[:, i].isna().sum()
            missing_rate = missing_count / total_rows

            missing_analysis["column_missing_stats"][col_name] = {
                "missing_count": int(missing_count),
                "missing_rate": float(missing_rate),
                "data_type": str(self.trades_df.iloc[:, i].dtype),
                "unique_values": int(self.trades_df.iloc[:, i].nunique()),
            }

        # ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        non_null_rates = [
            1 - stats["missing_rate"]
            for stats in missing_analysis["column_missing_stats"].values()
        ]
        missing_analysis["data_completeness_score"] = float(np.mean(non_null_rates))

        # ç³»çµ±çš„æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        # é‡è¦åˆ—ã§ã®æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        important_cols = [0, 1, 6, 12]  # æ™‚åˆ»ã€æ³¨æ–‡ã€ä¾¡æ ¼ã€ã‚³ãƒ¡ãƒ³ãƒˆ

        systematic_missing = {}
        for col_idx in important_cols:
            if col_idx < len(self.trades_df.columns):
                col_data = self.trades_df.iloc[:, col_idx]
                missing_mask = col_data.isna()

                if missing_mask.sum() > 0:
                    # æ¬ æã®é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯
                    missing_runs = []
                    current_run = 0
                    for is_missing in missing_mask:
                        if is_missing:
                            current_run += 1
                        else:
                            if current_run > 0:
                                missing_runs.append(current_run)
                            current_run = 0
                    if current_run > 0:
                        missing_runs.append(current_run)

                    systematic_missing[f"column_{col_idx}"] = {
                        "missing_runs": missing_runs[:10],  # æœ€åˆã®10å€‹
                        "max_consecutive_missing": max(missing_runs)
                        if missing_runs
                        else 0,
                        "total_missing_blocks": len(missing_runs),
                    }

        missing_analysis["systematic_missing_evidence"] = systematic_missing

        self.logger.info(
            f"ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã‚¹ã‚³ã‚¢: {missing_analysis['data_completeness_score']:.3f}"
        )

        return missing_analysis

    def verify_logical_consistency(self) -> Dict:
        """è«–ç†çš„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        self.logger.info("=== è«–ç†çš„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ ===")

        logical_analysis = {
            "position_id_consistency": {},
            "comment_logic_consistency": {},
            "price_data_consistency": {},
            "temporal_logic_violations": [],
        }

        # åˆ—å®šç¾©
        self.trades_df.columns[0]
        order_col = self.trades_df.columns[1]
        price_col = self.trades_df.columns[6]
        comment_col = self.trades_df.columns[12]

        # position_id ã®è«–ç†æ€§ãƒã‚§ãƒƒã‚¯
        position_ids = pd.to_numeric(
            self.trades_df[order_col], errors="coerce"
        ).dropna()

        logical_analysis["position_id_consistency"] = {
            "sequential_ids": len(position_ids) > 0,
            "id_range": [int(position_ids.min()), int(position_ids.max())]
            if len(position_ids) > 0
            else [None, None],
            "missing_ids_count": 0,
            "duplicate_ids_count": int(position_ids.duplicated().sum()),
        }

        # ID ã®é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯
        if len(position_ids) > 0:
            id_set = set(position_ids.astype(int))
            expected_range = set(
                range(int(position_ids.min()), int(position_ids.max()) + 1)
            )
            missing_ids = expected_range - id_set
            logical_analysis["position_id_consistency"]["missing_ids_count"] = len(
                missing_ids
            )

        # ã‚³ãƒ¡ãƒ³ãƒˆãƒ­ã‚¸ãƒƒã‚¯æ•´åˆæ€§
        comments = self.trades_df[comment_col].astype(str)
        jamesorb_count = comments.str.contains("JamesORB", na=False).sum()
        sl_count = comments.str.contains("sl ", na=False).sum()
        tp_count = comments.str.contains("tp ", na=False).sum()

        logical_analysis["comment_logic_consistency"] = {
            "jamesorb_entries": int(jamesorb_count),
            "stop_loss_exits": int(sl_count),
            "take_profit_exits": int(tp_count),
            "entry_exit_ratio": float((sl_count + tp_count) / jamesorb_count)
            if jamesorb_count > 0
            else 0,
            "expected_ratio_range": [0.8, 1.2],  # æœŸå¾…ã•ã‚Œã‚‹æ¯”ç‡ç¯„å›²
            "ratio_within_expected": 0.8
            <= ((sl_count + tp_count) / jamesorb_count)
            <= 1.2
            if jamesorb_count > 0
            else False,
        }

        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
        prices = pd.to_numeric(self.trades_df[price_col], errors="coerce").dropna()

        if len(prices) > 0:
            logical_analysis["price_data_consistency"] = {
                "valid_price_count": len(prices),
                "price_range": [float(prices.min()), float(prices.max())],
                "zero_prices": int((prices == 0).sum()),
                "negative_prices": int((prices < 0).sum()),
                "abnormal_prices": int(
                    ((prices < 0.5) | (prices > 5.0)).sum()
                ),  # EURUSD ã®ç•°å¸¸å€¤
                "price_precision_issues": int(
                    (prices * 100000 % 1 != 0).sum()
                ),  # 5æ¡ç²¾åº¦ãƒã‚§ãƒƒã‚¯
            }

        self.logger.info(
            f"position_idé‡è¤‡: {logical_analysis['position_id_consistency']['duplicate_ids_count']}å€‹"
        )
        self.logger.info(
            f"ã‚¨ãƒ³ãƒˆãƒªãƒ¼/ã‚¨ã‚°ã‚¸ãƒƒãƒˆæ¯”ç‡: {logical_analysis['comment_logic_consistency']['entry_exit_ratio']:.2f}"
        )

        return logical_analysis

    def generate_corruption_hypothesis_verdict(
        self,
        duplicate_analysis: Dict,
        integrity_analysis: Dict,
        missing_analysis: Dict,
        logical_analysis: Dict,
    ) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬æœ€çµ‚åˆ¤å®š"""

        verdict = {
            "corruption_evidence_score": 0.0,
            "corruption_indicators": [],
            "data_quality_issues": [],
            "confidence_level": "LOW",
            "final_conclusion": "",
            "severity_assessment": {},
        }

        evidence_scores = []

        # 1. é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®æ·±åˆ»åº¦
        if "duplicate_statistics" in duplicate_analysis:
            dup_stats = duplicate_analysis["duplicate_statistics"]
            dup_rate = dup_stats["duplicate_timestamps"] / dup_stats["total_timestamps"]

            if dup_rate > 0.1:  # 10%ä»¥ä¸Šé‡è¤‡
                evidence_scores.append(0.8)
                verdict["corruption_indicators"].append(
                    f"å¤§é‡ã®é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— ({dup_rate:.1%})"
                )
            elif dup_rate > 0.05:
                evidence_scores.append(0.6)
                verdict["corruption_indicators"].append(
                    f"ä¸­ç¨‹åº¦ã®é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— ({dup_rate:.1%})"
                )
            else:
                evidence_scores.append(0.2)

        # 2. æ™‚ç³»åˆ—é †åºé•åã®æ·±åˆ»åº¦
        if "reverse_sequences" in integrity_analysis:
            reverse_rate = integrity_analysis["reverse_sequences"][
                "reverse_sequence_rate"
            ]

            if reverse_rate > 0.3:  # 30%ä»¥ä¸Šé€†é †
                evidence_scores.append(0.9)
                verdict["corruption_indicators"].append(
                    f"å¤§é‡ã®æ™‚ç³»åˆ—é †åºé•å ({reverse_rate:.1%})"
                )
            elif reverse_rate > 0.1:
                evidence_scores.append(0.7)
                verdict["corruption_indicators"].append(
                    f"ä¸­ç¨‹åº¦ã®æ™‚ç³»åˆ—é †åºé•å ({reverse_rate:.1%})"
                )
            else:
                evidence_scores.append(0.3)

        # 3. ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã®å•é¡Œ
        completeness_score = missing_analysis.get("data_completeness_score", 1.0)
        if completeness_score < 0.8:
            evidence_scores.append(0.7)
            verdict["data_quality_issues"].append(
                f"ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ä½ä¸‹ ({completeness_score:.1%})"
            )
        elif completeness_score < 0.95:
            evidence_scores.append(0.4)
            verdict["data_quality_issues"].append(
                f"è»½å¾®ãªãƒ‡ãƒ¼ã‚¿æ¬ æ ({completeness_score:.1%})"
            )
        else:
            evidence_scores.append(0.1)

        # 4. è«–ç†çš„æ•´åˆæ€§ã®å•é¡Œ
        if "comment_logic_consistency" in logical_analysis:
            logic = logical_analysis["comment_logic_consistency"]
            if not logic["ratio_within_expected"]:
                evidence_scores.append(0.6)
                verdict["data_quality_issues"].append(
                    f"ã‚¨ãƒ³ãƒˆãƒªãƒ¼/ã‚¨ã‚°ã‚¸ãƒƒãƒˆæ¯”ç‡ç•°å¸¸ ({logic['entry_exit_ratio']:.2f})"
                )
            else:
                evidence_scores.append(0.2)

        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        if evidence_scores:
            verdict["corruption_evidence_score"] = sum(evidence_scores) / len(
                evidence_scores
            )

        # ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«æ±ºå®š
        if verdict["corruption_evidence_score"] >= 0.8:
            verdict["confidence_level"] = "VERY_HIGH"
            verdict["final_conclusion"] = "ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬ã¯æ¥µã‚ã¦å¦¥å½“"
        elif verdict["corruption_evidence_score"] >= 0.6:
            verdict["confidence_level"] = "HIGH"
            verdict["final_conclusion"] = "ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬ã¯å¦¥å½“"
        elif verdict["corruption_evidence_score"] >= 0.4:
            verdict["confidence_level"] = "MODERATE"
            verdict["final_conclusion"] = "ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬ã¯éƒ¨åˆ†çš„ã«å¦¥å½“"
        else:
            verdict["confidence_level"] = "LOW"
            verdict["final_conclusion"] = "ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬ã¯ç–‘ã‚ã—ã„"

        # æ·±åˆ»åº¦è©•ä¾¡
        verdict["severity_assessment"] = {
            "data_unusable": verdict["corruption_evidence_score"] > 0.8,
            "requires_cleaning": verdict["corruption_evidence_score"] > 0.6,
            "minor_issues_only": verdict["corruption_evidence_score"] < 0.4,
        }

        return verdict

    def run_comprehensive_corruption_verification(self) -> Dict:
        """åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬æ¤œè¨¼å®Ÿè¡Œ"""
        self.logger.info("ğŸ”§ === MT5ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬åŒ…æ‹¬æ¤œè¨¼é–‹å§‹ ===")

        if not self.load_data_for_corruption_analysis():
            return {"error": "Failed to load data"}

        # æ¤œè¨¼1: é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æ
        duplicate_analysis = self.analyze_duplicate_timestamps()

        # æ¤œè¨¼2: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †åºæ•´åˆæ€§
        integrity_analysis = self.verify_timestamp_sequence_integrity()

        # æ¤œè¨¼3: ãƒ‡ãƒ¼ã‚¿æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³
        missing_analysis = self.analyze_data_missing_patterns()

        # æ¤œè¨¼4: è«–ç†çš„æ•´åˆæ€§
        logical_analysis = self.verify_logical_consistency()

        # æœ€çµ‚åˆ¤å®š
        final_verdict = self.generate_corruption_hypothesis_verdict(
            duplicate_analysis, integrity_analysis, missing_analysis, logical_analysis
        )

        # çµæœçµ±åˆ
        comprehensive_result = {
            "timestamp": datetime.now().isoformat(),
            "verification_method": "MT5_Data_Corruption_Hypothesis_Comprehensive_Verification",
            "duplicate_timestamp_analysis": duplicate_analysis,
            "timestamp_integrity_analysis": integrity_analysis,
            "data_missing_analysis": missing_analysis,
            "logical_consistency_analysis": logical_analysis,
            "final_verdict": final_verdict,
        }

        # çµæœä¿å­˜
        output_path = "/home/trader/Trading-Development/2.ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ‰‹æ³•ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/MT5_Results/data_corruption_verification.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(
                comprehensive_result, f, indent=2, ensure_ascii=False, default=str
            )

        self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬æ¤œè¨¼çµæœä¿å­˜: {output_path}")
        self.logger.info(f"ğŸ¯ æœ€çµ‚çµè«–: {final_verdict['final_conclusion']}")
        self.logger.info(f"ğŸ“Š ä¿¡é ¼åº¦: {final_verdict['confidence_level']}")
        self.logger.info(f"ğŸ”¢ ç ´æè¨¼æ‹ ã‚¹ã‚³ã‚¢: {final_verdict['corruption_evidence_score']:.3f}")

        return comprehensive_result


def main():
    """ãƒ‡ãƒ¼ã‚¿ç ´æä»®èª¬æ¤œè¨¼å®Ÿè¡Œ"""
    excel_path = "/home/trader/Trading-Development/2.ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ‰‹æ³•ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/MT5_Results/Reportãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ-900179988.xlsx"

    verifier = MT5DataCorruptionVerification(excel_path)
    results = verifier.run_comprehensive_corruption_verification()

    return results


if __name__ == "__main__":
    main()
